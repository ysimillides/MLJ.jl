var documenterSearchIndex = {"docs":
[{"location":"NEWS/#MLJ-News-1","page":"News","title":"MLJ News","text":"","category":"section"},{"location":"NEWS/#","page":"News","title":"News","text":"Development news for MLJ and its satellite packages,  MLJBase, MLJRegistry and MLJModels","category":"page"},{"location":"NEWS/#unversioned-commits-1-March-2019-(some-time-after-03:50-GMT)-1","page":"News","title":"unversioned commits 1 March 2019 (some time after 03:50 GMT)","text":"","category":"section"},{"location":"NEWS/#","page":"News","title":"News","text":"Addition of \"scientific type\" hierarchy, including Continuous, Discrete, Multiclass, and Other subtypes of Found (to complement Missing). See new documents Getting Started and Scientific Data Types for more one this.  Resolves: #86\nRevamp of model traits to take advantage of scientific types, with output_kind replaced with target_scitype, input_kind replaced with input_scitype. Also, output_quantity dropped, input_quantity replaced with Bool-valued input_is_multivariate, and is_pure_julia made Bool-valued. Trait definitions in all model implementations and effected meta-algorithms have been updated. Related: #81\nSubstantial update of the core guide Adding New Models to reflect above changes and in response to new model implementer queries. Some design \"decisions\" regarding multivariate targets now explict there.\nthe order the y and yhat arguments of measures (aka loss functions) have been reversed. Progress on: #91\nUpdate of Standardizer and OneHotEncoder to mesh with new scitypes.\nNew improved task constructors infer task metadata from data scitypes. This brings us close to a simple implementation of basic task-model matching. Query the doc-strings for SupervisedTask and UnsupervisedTask for details.  Machines can now dispatch on tasks instead of X and y. A task, task, is now callable: task() returns (X, y) for supervised models, and X for unsupervised models.  Progress on:  #86\nthe data in the load_ames() test task has been replaced by the full data set, and load_reduced_ames() now loads a reduced set.","category":"page"},{"location":"getting_started/#Getting-Started-1","page":"Getting Started","title":"Getting Started","text":"","category":"section"},{"location":"getting_started/#Basic-supervised-training-and-testing-1","page":"Getting Started","title":"Basic supervised training and testing","text":"","category":"section"},{"location":"getting_started/#","page":"Getting Started","title":"Getting Started","text":"julia> using Revise\njulia> using MLJ\njulia> using RDatasets\njulia> iris = dataset(\"datasets\", \"iris\"); # a DataFrame","category":"page"},{"location":"getting_started/#","page":"Getting Started","title":"Getting Started","text":"┌ Info: Recompiling stale cache file /Users/anthony/.julia/compiled/v1.0/MLJ/rAU56.ji for MLJ [add582a8-e3ab-11e8-2d5e-e98b27df1bc7]\n└ @ Base loading.jl:1190","category":"page"},{"location":"getting_started/#","page":"Getting Started","title":"Getting Started","text":"In MLJ one can either wrap data for supervised learning in a formal task (see Working with Tasks), or work directly with the data, split into its input and target parts:","category":"page"},{"location":"getting_started/#","page":"Getting Started","title":"Getting Started","text":"julia> const X = iris[:, 1:4];\njulia> const y = iris[:, 5];","category":"page"},{"location":"getting_started/#","page":"Getting Started","title":"Getting Started","text":"A model is a container for hyperparameters:","category":"page"},{"location":"getting_started/#","page":"Getting Started","title":"Getting Started","text":"julia> @load DecisionTreeClassifier\njulia> tree_model = DecisionTreeClassifier(target_type=String, max_depth=2)","category":"page"},{"location":"getting_started/#","page":"Getting Started","title":"Getting Started","text":"import MLJModels ✔\nimport DecisionTree ✔\nimport MLJModels.DecisionTree_.DecisionTreeClassifier ✔\n\n# DecisionTreeClassifier{String} @ 2…98: \ntarget_type             =>   String\npruning_purity          =>   1.0\nmax_depth               =>   2\nmin_samples_leaf        =>   1\nmin_samples_split       =>   2\nmin_purity_increase     =>   0.0\nn_subfeatures           =>   0.0\ndisplay_depth           =>   5\npost_prune              =>   false\nmerge_purity_threshold  =>   0.9","category":"page"},{"location":"getting_started/#","page":"Getting Started","title":"Getting Started","text":"Wrapping the model in data creates a machine which will store training outcomes (called fit-results):","category":"page"},{"location":"getting_started/#","page":"Getting Started","title":"Getting Started","text":"julia> tree = machine(tree_model, X, y)","category":"page"},{"location":"getting_started/#","page":"Getting Started","title":"Getting Started","text":"# Machine{DecisionTreeClassifier{S…} @ 1…36: \nmodel                   =>   DecisionTreeClassifier{String} @ 2…98\nfitresult               =>   (undefined)\ncache                   =>   (undefined)\nargs                    =>   (omitted Tuple{DataFrame,CategoricalArray{String,1,UInt8,String,CategoricalString{UInt8},Union{}}} of length 2)\nreport                  =>   empty Dict{Symbol,Any}\nrows                    =>   (undefined)","category":"page"},{"location":"getting_started/#","page":"Getting Started","title":"Getting Started","text":"Training and testing on a hold-out set:","category":"page"},{"location":"getting_started/#","page":"Getting Started","title":"Getting Started","text":"julia> train, test = partition(eachindex(y), 0.7, shuffle=true); # 70:30 split\njulia> fit!(tree, rows=train)\njulia> yhat = predict(tree, X[test,:]);\njulia> misclassification_rate(yhat, y[test]);","category":"page"},{"location":"getting_started/#","page":"Getting Started","title":"Getting Started","text":"┌ Info: Training Machine{DecisionTreeClassifier{S…} @ 1…36.\n└ @ MLJ /Users/anthony/Dropbox/Julia7/MLJ/src/machines.jl:68","category":"page"},{"location":"getting_started/#","page":"Getting Started","title":"Getting Started","text":"Or, in one line:","category":"page"},{"location":"getting_started/#","page":"Getting Started","title":"Getting Started","text":"julia> evaluate!(tree, resampling=Holdout(fraction_train=0.7, shuffle=true), measure=misclassification_rate)","category":"page"},{"location":"getting_started/#","page":"Getting Started","title":"Getting Started","text":"0.08888888888888889","category":"page"},{"location":"getting_started/#","page":"Getting Started","title":"Getting Started","text":"Changing a hyperparameter and re-evaluating:","category":"page"},{"location":"getting_started/#","page":"Getting Started","title":"Getting Started","text":"julia> tree_model.max_depth = 3\njulia> evaluate!(tree, resampling=Holdout(fraction_train=0.5, shuffle=true), measure=misclassification_rate)","category":"page"},{"location":"getting_started/#","page":"Getting Started","title":"Getting Started","text":"0.06666666666666667","category":"page"},{"location":"getting_started/#Next-steps-1","page":"Getting Started","title":"Next steps","text":"","category":"section"},{"location":"getting_started/#","page":"Getting Started","title":"Getting Started","text":"To learn a little more about what MLJ can do, take the MLJ tour. However, before considering serious data science applications, please acquaint yourselves with the remaining sections below.","category":"page"},{"location":"getting_started/#Prerequisites-1","page":"Getting Started","title":"Prerequisites","text":"","category":"section"},{"location":"getting_started/#","page":"Getting Started","title":"Getting Started","text":"MLJ assumes some familiarity with the CategoricalArrays.jl package, used for representing categorical data. For probabilistic predictors, a basic acquaintance with Distributions.jl is also assumed.","category":"page"},{"location":"getting_started/#Data-1","page":"Getting Started","title":"Data","text":"","category":"section"},{"location":"getting_started/#","page":"Getting Started","title":"Getting Started","text":"While MLJ is data container agnostic it is very fussy about element types. Every serious user needs to understand the basic assumptions about the form of data expected by MLJ, as outlined below.","category":"page"},{"location":"getting_started/#","page":"Getting Started","title":"Getting Started","text":"On the one hand, MLJ is quite flexible about how tabular data is presented: Anywhere a table is expected (eg, X above) any tabular format supporting the Tables.jl API is allowed. For example, DataFrame objects are supported. A single feature (such as the target y above) is expected to be a Vector or CategoricalVector, according to the scientific type of the data (see below). A multivariate target can be any table.","category":"page"},{"location":"getting_started/#","page":"Getting Started","title":"Getting Started","text":"On the other hand, the element types you use to represent your data has implicit consequences about how MLJ will interpret that data.","category":"page"},{"location":"getting_started/#","page":"Getting Started","title":"Getting Started","text":"To articulate MLJ's conventions about data representation, MLJ distinguishes between machine data types on the one hand (Float64, Bool, String, etc) and scientific data types on the other, represented by new Julia types: Continuous, Discrete, Multiclass{N}, FiniteOrderedFactor{N}, and Count, with obvious interpretations. These types, which are organized in a type hierarchy (see Scientific Data Types), are used by MLJ for dispatch (but have no corresponding instances).","category":"page"},{"location":"getting_started/#","page":"Getting Started","title":"Getting Started","text":"Scientific types appear when querying model metadata, as in this example:","category":"page"},{"location":"getting_started/#","page":"Getting Started","title":"Getting Started","text":"julia> info(\"DecisionTreeClassifier\")[:target_scitype]","category":"page"},{"location":"getting_started/#","page":"Getting Started","title":"Getting Started","text":"Multiclass","category":"page"},{"location":"getting_started/#","page":"Getting Started","title":"Getting Started","text":"Basic data convention The scientific type of data that a Julia object x can represent is defined by scitype(x). If scitype(x) == Other, then x cannot represent scalar data in MLJ.","category":"page"},{"location":"getting_started/#","page":"Getting Started","title":"Getting Started","text":"julia> (scitype(42), scitype(π), scitype(String))","category":"page"},{"location":"getting_started/#","page":"Getting Started","title":"Getting Started","text":"(Count, Continuous, MLJBase.Other)","category":"page"},{"location":"getting_started/#","page":"Getting Started","title":"Getting Started","text":"In particular, integers cannot be used to represent Multiclass or FiniteOrderedFactor data; these can represented by an unordered or ordered CategoricalValue or CategoricalString:","category":"page"},{"location":"getting_started/#","page":"Getting Started","title":"Getting Started","text":"T scitype(x) for x::T\nMissing Missing\nReal Continuous\nInteger Count\nCategoricalValue Multiclass{N} where N = nlevels(x), provided x.pool.ordered == false\nCategoricalString Multiclass{N} where N = nlevels(x), provided x.pool.ordered == false\nCategoricalValue FiniteOrderedFactor{N} where N = nlevels(x), provided x.pool.ordered == true\nCategoricalString FiniteOrderedFactor{N} where N = nlevels(x) provided x.pool.ordered == true\nInteger Count","category":"page"},{"location":"getting_started/#","page":"Getting Started","title":"Getting Started","text":"Here nlevels(x) = length(levels(x.pool)).","category":"page"},{"location":"glossary/#Glossary-1","page":"Glossary","title":"Glossary","text":"","category":"section"},{"location":"glossary/#","page":"Glossary","title":"Glossary","text":"Note that this glossary refers to some objects not accessed by the general user and is intended mainly for MLJ developers.","category":"page"},{"location":"glossary/#Basics-1","page":"Glossary","title":"Basics","text":"","category":"section"},{"location":"glossary/#task-(object-of-type-Task)-1","page":"Glossary","title":"task (object of type Task)","text":"","category":"section"},{"location":"glossary/#","page":"Glossary","title":"Glossary","text":"Data plus a learning objective (e.g., \"probabilistic prediction of Sales\"). In MLJ a task does not include a description of how the completed task is to be evaluated.","category":"page"},{"location":"glossary/#hyperparameters-1","page":"Glossary","title":"hyperparameters","text":"","category":"section"},{"location":"glossary/#","page":"Glossary","title":"Glossary","text":"Parameters on which some learning algorithm depends, specified before the algorithm is applied, and where learning is interpreted in the broadest sense. For example, PCA feature reduction is a \"preprocessing\" transformation \"learning\" a projection from training data, governed by a dimension hyperparameter. Hyperparameters in our sense may specify configuration (eg, number of parallel processes) even when this does not effect the end-product of learning. (But we exlcude verbosity level.)","category":"page"},{"location":"glossary/#model-(object-of-abstract-type-Model)-1","page":"Glossary","title":"model (object of abstract type Model)","text":"","category":"section"},{"location":"glossary/#","page":"Glossary","title":"Glossary","text":"Object collecting together hyperameters of a single algorithm. Most models are classified either as supervised or unsupervised models (generally, \"transformers\").","category":"page"},{"location":"glossary/#fit-result-(type-generally-defined-outside-of-MLJ)-1","page":"Glossary","title":"fit-result (type generally defined outside of MLJ)","text":"","category":"section"},{"location":"glossary/#","page":"Glossary","title":"Glossary","text":"Also known as \"learned\" or \"fitted\" parameters, these are \"weights\", \"coefficients\", or similar paramaters learned by an algorithm, after adopting the prescribed hyperparameters. For example, decision trees of a random forest, the coefficients and intercept of a linear model, or the rotation and projection matrices of PCA reduction scheme.","category":"page"},{"location":"glossary/#","page":"Glossary","title":"Glossary","text":"<!– #### method –>","category":"page"},{"location":"glossary/#","page":"Glossary","title":"Glossary","text":"<!– What Julia calls a function. (In Julia, a \"function\" is a collection –> <!– of methods sharing the same name but different type signatures.) –>","category":"page"},{"location":"glossary/#","page":"Glossary","title":"Glossary","text":"<!– For example, associated with every machine (see below) is a fit! method for computing assoicated –> <!– fit-results. –>","category":"page"},{"location":"glossary/#operation-1","page":"Glossary","title":"operation","text":"","category":"section"},{"location":"glossary/#","page":"Glossary","title":"Glossary","text":"Data-manipulating operations (methods) parameterized by some fit-result. For supervised learners, the predict or predict_mode methods, for transformers, the transform or inverse_transform method. In some contexts, such an operation might be replaced by an ordinary operation (method) that does not depend on an fit-result, which are then then called static operations for clarity. An operation that is not static is dynamic.","category":"page"},{"location":"glossary/#machine-(object-of-type-Machine)-1","page":"Glossary","title":"machine (object of type Machine)","text":"","category":"section"},{"location":"glossary/#","page":"Glossary","title":"Glossary","text":"An object consisting of:","category":"page"},{"location":"glossary/#","page":"Glossary","title":"Glossary","text":"(1) A model ","category":"page"},{"location":"glossary/#","page":"Glossary","title":"Glossary","text":"(2) A fit-result (undefined until training)","category":"page"},{"location":"glossary/#","page":"Glossary","title":"Glossary","text":"(3) Training arguments (one for each data argument of the model's associated fit method). A training argument is data used for training. Generally, there are two training arguments for supervised models, and just one for unsuperivsed models.","category":"page"},{"location":"glossary/#","page":"Glossary","title":"Glossary","text":"In addition machines store \"report\" metadata, for recording algorithm-specific statistics of training (eg, internal estimate of generalization error, feature importances); and they cache information allowing the fit-result to be updated without repeating unnecessary information.","category":"page"},{"location":"glossary/#","page":"Glossary","title":"Glossary","text":"Trainable models are trained by calls to a fit method which may be passed an optional argument specifying the rows of data to be used in training.","category":"page"},{"location":"glossary/#Learning-Networks-and-Composite-Models-1","page":"Glossary","title":"Learning Networks and Composite Models","text":"","category":"section"},{"location":"glossary/#","page":"Glossary","title":"Glossary","text":"Note: Multiple nodal machines may share the same model, and multiple learning nodes may share the same nodal machine.","category":"page"},{"location":"glossary/#source-node-(object-of-type-'Source')-1","page":"Glossary","title":"source node (object of type 'Source')","text":"","category":"section"},{"location":"glossary/#","page":"Glossary","title":"Glossary","text":"A container for training data and point of entry for new data in a learning network (see below).","category":"page"},{"location":"glossary/#nodal-machine-(object-of-type-'NodalMachine')-1","page":"Glossary","title":"nodal machine (object of type 'NodalMachine')","text":"","category":"section"},{"location":"glossary/#","page":"Glossary","title":"Glossary","text":"Like a machine with the following exceptions:","category":"page"},{"location":"glossary/#","page":"Glossary","title":"Glossary","text":"(1) Training arguments are source nodes or regular nodes in the learning network, instead of data.","category":"page"},{"location":"glossary/#","page":"Glossary","title":"Glossary","text":"(2) The object internally records dependencies on other other nodal machines, as implied by the training arguments, and so on. ","category":"page"},{"location":"glossary/#node-(object-of-type-'Node')-1","page":"Glossary","title":"node (object of type 'Node')","text":"","category":"section"},{"location":"glossary/#","page":"Glossary","title":"Glossary","text":"Essentially a nodal machine wrapped in an assoicated operation (e.g., predict or inverse_transform. It detail, it consists of:","category":"page"},{"location":"glossary/#","page":"Glossary","title":"Glossary","text":"(1) An operation, static or dynamic.","category":"page"},{"location":"glossary/#","page":"Glossary","title":"Glossary","text":"(2) A nodal machine, void if the operation is static.","category":"page"},{"location":"glossary/#","page":"Glossary","title":"Glossary","text":"(3) Upstream connections to other learning or source nodes, specified by a list    of arguments (one for each argument of the operation).","category":"page"},{"location":"glossary/#","page":"Glossary","title":"Glossary","text":"(4) Metadata recording the dependencies of the object's machine, and the dependecies on other nodal machines implied by its arguments.","category":"page"},{"location":"glossary/#learning-network-1","page":"Glossary","title":"learning network","text":"","category":"section"},{"location":"glossary/#","page":"Glossary","title":"Glossary","text":"An acyclic directed graph implicit in the connections of a collection of source(s) and nodes. Each connected component is ordinarily restricted to have a unique source.","category":"page"},{"location":"glossary/#composite-model-1","page":"Glossary","title":"composite model","text":"","category":"section"},{"location":"glossary/#","page":"Glossary","title":"Glossary","text":"Any model with one or more other models as hyperparameters. ","category":"page"},{"location":"internals/#Internals-1","page":"Internals","title":"Internals","text":"","category":"section"},{"location":"internals/#The-machine-interface,-simplified-1","page":"Internals","title":"The machine interface, simplified","text":"","category":"section"},{"location":"internals/#","page":"Internals","title":"Internals","text":"The following is simplified description of the Machine interface. See also the glossary","category":"page"},{"location":"internals/#The-Machine-type-1","page":"Internals","title":"The Machine type","text":"","category":"section"},{"location":"internals/#","page":"Internals","title":"Internals","text":"mutable struct Machine{M<Model}\n\n\tmodel::M\n    fitresult\n    cache\n    args::Tuple    # e.g., (X, y) for supervised models\n    report\n    rows # remember last rows used \n    \n    function Machine{M}(model::M, args...) where M<:Model\n        machine = new{M}(model)\n        machine.args = args\n        machine.report = Dict{Symbol,Any}()\n        return machine\n    end\n\nend","category":"page"},{"location":"internals/#Constructor-1","page":"Internals","title":"Constructor","text":"","category":"section"},{"location":"internals/#","page":"Internals","title":"Internals","text":"machine(model::M, Xtable, y) = Machine{M}(model, Xtable, y)","category":"page"},{"location":"internals/#fit!-and-predict/transform-1","page":"Internals","title":"fit! and predict/transform","text":"","category":"section"},{"location":"internals/#","page":"Internals","title":"Internals","text":"function fit!(machine::Machine; rows=nothing, verbosity=1) \n\n    warning = clean!(mach.model)\n    isempty(warning) || verbosity < 0 || @warn warning \n\n    if rows == nothing\n        rows = (:) \n    end\n\n    rows_have_changed  = (!isdefined(mach, :rows) || rows != mach.rows)\n\n    args = [MLJ.selectrows(arg, rows) for arg in mach.args]\n\t\n    if !isdefined(mach, :fitresult) || rows_have_changed || force \n        mach.fitresult, mach.cache, report =\n            fit(mach.model, verbosity, args...)\n    else # call `update`:\n        mach.fitresult, mach.cache, report =\n            update(mach.model, verbosity, mach.fitresult, mach.cache, args...)\n    end\n\n    if rows_have_changed\n        mach.rows = deepcopy(rows)\n    end\n\n    if report != nothing\n        merge!(mach.report, report)\n    end\n\n    return mach\n\nend\n\nfunction predict(machine::Machine{<:Supervised}, Xnew)\n    if isdefined(machine, :fitresult)\n        return predict(machine.model, machine.fitresult, Xnew))\n    else\n        throw(error(\"$machine is not trained and so cannot predict.\"))\n    end\nend\n\nfunction transform(machine::Machine{<:Unsupervised}, Xnew)\n    if isdefined(machine, :fitresult)\n        return transform(machine.model, machine.fitresult, Xnew))\n    else\n        throw(error(\"$machine is not trained and so cannot transform.\"))\n    end\nend","category":"page"},{"location":"adding_new_models/#Adding-New-Models-1","page":"Adding new Models","title":"Adding New Models","text":"","category":"section"},{"location":"adding_new_models/#","page":"Adding new Models","title":"Adding new Models","text":"This guide outlines the specification of the MLJ model interface and provides guidelines for implementing the interface for models defined in external packages. For sample implementations, see MLJModels/src.","category":"page"},{"location":"adding_new_models/#","page":"Adding new Models","title":"Adding new Models","text":"The machine learning tools provided by MLJ can be applied to the models in any package that imports the package MLJBase and implements the API defined there, as outlined below. For a quick and dirty implementation of user-defined models see here.  To make new models available to all MLJ users, see Where to place code implementing new models below.","category":"page"},{"location":"adding_new_models/#","page":"Adding new Models","title":"Adding new Models","text":"It is assumed the reader has read Getting Started. To implement the API described here, some familiarity with the following packages is also helpful:","category":"page"},{"location":"adding_new_models/#","page":"Adding new Models","title":"Adding new Models","text":"Distributions.jl","category":"page"},{"location":"adding_new_models/#","page":"Adding new Models","title":"Adding new Models","text":"(for probabilistic predictions)","category":"page"},{"location":"adding_new_models/#","page":"Adding new Models","title":"Adding new Models","text":"CategoricalArrays.jl","category":"page"},{"location":"adding_new_models/#","page":"Adding new Models","title":"Adding new Models","text":"(essential if you are implementing a model handling data of Multiclass or FiniteOrderedFactor scitype)","category":"page"},{"location":"adding_new_models/#","page":"Adding new Models","title":"Adding new Models","text":"Tables.jl (if you're","category":"page"},{"location":"adding_new_models/#","page":"Adding new Models","title":"Adding new Models","text":"algorithm needs input data in a novel format).","category":"page"},{"location":"adding_new_models/#","page":"Adding new Models","title":"Adding new Models","text":"<!– As a temporary measure, –> <!– the MLJ package also implements the MLJ interface for some –> <!– non-compliant packages, using lazily loaded modules (\"glue code\") –> <!– residing in –> <!– src/interfaces –> <!– of the MLJ.jl repository. A checklist for adding models in this latter –> <!– way is given at the end; a template is given here: –> <!– \"src/interfaces/DecisionTree.jl\". –>","category":"page"},{"location":"adding_new_models/#","page":"Adding new Models","title":"Adding new Models","text":"In MLJ, the basic interface exposed to the user, built atop the model interface described here, is the machine interface. After a first reading of this document, the reader may wish to refer to MLJ Internals for context.","category":"page"},{"location":"adding_new_models/#Overview-1","page":"Adding new Models","title":"Overview","text":"","category":"section"},{"location":"adding_new_models/#","page":"Adding new Models","title":"Adding new Models","text":"A model is an object storing hyperparameters associated with some machine learning algorithm, where \"learning algorithm\" is broadly interpreted.  In MLJ, hyperparameters include configuration parameters, like the number of threads, and special instructions, such as \"compute feature rankings\", which may or may not affect the final learning outcome.  However, the logging level (verbosity below) is excluded.","category":"page"},{"location":"adding_new_models/#","page":"Adding new Models","title":"Adding new Models","text":"The name of the Julia type associated with a model indicates the associated algorithm (e.g., DecisionTreeClassifier). The outcome of training a learning algorithm is here called a fit-result. For ordinary multilinear regression, for example, this would be the coefficients and intercept. For a general supervised model, it is the (generally minimal) information needed to make new predictions.","category":"page"},{"location":"adding_new_models/#","page":"Adding new Models","title":"Adding new Models","text":"The ultimate supertype of all models is MLJBase.Model, which has two abstract subtypes:","category":"page"},{"location":"adding_new_models/#","page":"Adding new Models","title":"Adding new Models","text":"abstract type Supervised{R} <: Model end\nabstract type Unsupervised <: Model end","category":"page"},{"location":"adding_new_models/#","page":"Adding new Models","title":"Adding new Models","text":"Here the parameter R refers to a fit-result type. By declaring a model to be a subtype of MLJBase.Supervised{R} you guarantee the fit-result to be of type R and, if R is concrete, one may improve the performance of homogeneous ensembles of the model (as defined by the built-in MLJ EnsembleModel wrapper). There is no abstract type for fit-results because these types are generally declared outside of MLJBase.","category":"page"},{"location":"adding_new_models/#","page":"Adding new Models","title":"Adding new Models","text":"The necessity to declare the fitresult type R may disappear in the future (issue #93).","category":"page"},{"location":"adding_new_models/#","page":"Adding new Models","title":"Adding new Models","text":"Supervised models are further divided according to whether they are able to furnish probabilistic predictions of the target(s) (which they will do so by default) or directly predict \"point\" estimates, for each new input pattern:","category":"page"},{"location":"adding_new_models/#","page":"Adding new Models","title":"Adding new Models","text":"abstract type Probabilistic{R} <: Supervised{R} end\nabstract type Deterministic{R} <: Supervised{R} end","category":"page"},{"location":"adding_new_models/#","page":"Adding new Models","title":"Adding new Models","text":"Further division of model types is realized through trait declarations.","category":"page"},{"location":"adding_new_models/#","page":"Adding new Models","title":"Adding new Models","text":"Associated with every concrete subtype of Model there must be a fit method, which implements the associated algorithm to produce the fit-result. Additionally, every Supervised model has a predict method, while Unsupervised models must have a transform method. More generally, methods such as these, that are dispatched on a model instance and a fit-result (plus other data), are called operations. Probabilistic supervised models optionally implement a predict_mode operation (in the case of classifiers) or a predict_mean and/or predict_median operations (in the case of regressors) overriding obvious fallbacks provided by MLJBase. Unsupervised models may implement an inverse_transform operation.","category":"page"},{"location":"adding_new_models/#New-model-type-declarations-and-optional-clean!-method-1","page":"Adding new Models","title":"New model type declarations and optional clean! method","text":"","category":"section"},{"location":"adding_new_models/#","page":"Adding new Models","title":"Adding new Models","text":"Here is an example of a concrete supervised model type declaration, made after defining an appropriate fit-result type (an optional step):","category":"page"},{"location":"adding_new_models/#","page":"Adding new Models","title":"Adding new Models","text":"import MLJ\n\nstruct LinearFitResult{F<:AbstractFloat} <: MLJBase.MLJType\n    coefficients::Vector{F}\n    bias::F\nend\n\nmutable struct RidgeRegressor{F} <: MLJBase.Deterministic{LinearFitResult{F}}\n    target_type::Type{F}\n    lambda::Float64\nend","category":"page"},{"location":"adding_new_models/#","page":"Adding new Models","title":"Adding new Models","text":"Models (which are mutable) should not be given internal constructors. It is recommended that they be given an external lazy keyword constructor of the same name. This constructor defines default values for every field, and optionally corrects invalid field values by calling a clean! method (whose fallback returns an empty message string):","category":"page"},{"location":"adding_new_models/#","page":"Adding new Models","title":"Adding new Models","text":"function MLJ.clean!(model::RidgeRegressor)\n    warning = \"\"\n    if model.lambda < 0\n        warning *= \"Need lambda ≥ 0. Resetting lambda=0. \"\n        model.lambda = 0\n    end\n    return warning\nend\n\n# keyword constructor\nfunction RidgeRegressor(; target_type=Float64, lambda=0.0)\n\n    model = RidgeRegressor(target_type, lambda)\n\n    message = MLJBase.clean!(model)\n    isempty(message) || @warn message\n\n    return model\n    \nend","category":"page"},{"location":"adding_new_models/#The-model-API-for-supervised-models-1","page":"Adding new Models","title":"The model API for supervised models","text":"","category":"section"},{"location":"adding_new_models/#","page":"Adding new Models","title":"Adding new Models","text":"Below we describe the compulsory and optional methods to be specified for each concrete type SomeSupervisedModel{R} <: MLJBase.Supervised{R}. ","category":"page"},{"location":"adding_new_models/#The-form-of-data-for-fitting-and-predicting-1","page":"Adding new Models","title":"The form of data for fitting and predicting","text":"","category":"section"},{"location":"adding_new_models/#","page":"Adding new Models","title":"Adding new Models","text":"In every circumstance, the argument X passed to the fit method described below, and the argument Xnew of the predict method, will be some table supporting the Tables.jl API. The interface implementer can control the scientific type of data appearing in X with an appropriate input_scitype declaration (see Trait Declarations below). If the core algorithm requires data in a different or more specific form, then fit will need to coerce the table into the form desired. To this end, MLJ provides the convenience method MLJBase.matrix; MLJBase.matrix(Xtable) is a two-dimensional Array{T} where T is the tightest common type of elements of Xtable, and Xtable is any table.","category":"page"},{"location":"adding_new_models/#","page":"Adding new Models","title":"Adding new Models","text":"Tables.jl has recently added a matrix method as well.","category":"page"},{"location":"adding_new_models/#","page":"Adding new Models","title":"Adding new Models","text":"Other convenience methods provided by MLJBase for handling tabular data are: selectrows, selectcols, schema (for extracting the size, names and eltypes of a table) and table (for materializing an abstract matrix, or named tuple of vectors, as a table matching a given prototype). Query the doc-strings for details.","category":"page"},{"location":"adding_new_models/#","page":"Adding new Models","title":"Adding new Models","text":"Note that generally the same type coercions applied to X by fit will need to be applied by predict to Xnew. ","category":"page"},{"location":"adding_new_models/#","page":"Adding new Models","title":"Adding new Models","text":"Important convention It is to be understood that the columns of the table X correspond to features and the rows to patterns.","category":"page"},{"location":"adding_new_models/#","page":"Adding new Models","title":"Adding new Models","text":"The form of the target data y passed to fit is constrained by the target_scitype trait declaration. All elements of y will satisfy scitype(y) <: target_scitype(SomeSupervisedModelType). Furthermore, for univariate targets, y is always a Vector or CategoricalVector, according to the value of the trait:","category":"page"},{"location":"adding_new_models/#","page":"Adding new Models","title":"Adding new Models","text":"target_scitype(SomeSupervisedModelType) type of y tightest known supertype of eltype(y)\nContinuous Vector Real\n<: Multiclass CategoricalVector Union{CategoricalString, CategoricalValue}\n<: FiniteOrderedFactor CategoricalVector Union{CategoricalString, CategoricalValue}\nCount Vector Integer","category":"page"},{"location":"adding_new_models/#","page":"Adding new Models","title":"Adding new Models","text":"So, for example, if your model is a binary classifier, you declare","category":"page"},{"location":"adding_new_models/#","page":"Adding new Models","title":"Adding new Models","text":"target_scitype(SomeSupervisedModelType)=Multiclass{2}","category":"page"},{"location":"adding_new_models/#","page":"Adding new Models","title":"Adding new Models","text":"If it can predict any number of classes, you might instead declare","category":"page"},{"location":"adding_new_models/#","page":"Adding new Models","title":"Adding new Models","text":"target_scitype(SomeSupervisedModelType)=Union{Multiclass, FiniteOrderedFactor}","category":"page"},{"location":"adding_new_models/#","page":"Adding new Models","title":"Adding new Models","text":"See also the table in Getting Started.","category":"page"},{"location":"adding_new_models/#","page":"Adding new Models","title":"Adding new Models","text":"For multivariate targets, y will be a table whose columns have the scitypes indicated in the Tuple type returned by target_scitype; for example, if you declare target_scitype(SomeSupervisedModelType) = Tuple{Continuous,Count}, then y will have two columns, the first with Real elements, the second with Integer elements.","category":"page"},{"location":"adding_new_models/#The-fit-method-1","page":"Adding new Models","title":"The fit method","text":"","category":"section"},{"location":"adding_new_models/#","page":"Adding new Models","title":"Adding new Models","text":"A compulsory fit method returns three objects:","category":"page"},{"location":"adding_new_models/#","page":"Adding new Models","title":"Adding new Models","text":"MLJBase.fit(model::SomeSupervisedModelType, verbosity::Int, X, y) -> fitresult, cache, report","category":"page"},{"location":"adding_new_models/#","page":"Adding new Models","title":"Adding new Models","text":"Note: The Int typing of verbosity cannot be omitted.","category":"page"},{"location":"adding_new_models/#","page":"Adding new Models","title":"Adding new Models","text":"fitresult::R is the fit-result in the sense above (which becomes an  argument for predict discussed below).\nreport is either a Dict{Symbol,Any} object, or nothing if  there is nothing to report. So for example, fit might declare  report[:feature_importances] = ....  Any training-related  statistics, such as internal estimates of the generalization  error, feature rankings, and coefficients in linear models, should  be returned in the report dictionary. How, or if, these are  generated should be controlled by hyperparameters (the fields of  model). Reports get merged with those generated by previous  calls to fit by MLJ.","category":"page"},{"location":"adding_new_models/#","page":"Adding new Models","title":"Adding new Models","text":"3.\tThe value of cache can be nothing, unless one is also defining an     update method (see below). The Julia type of cache is not presently restricted.","category":"page"},{"location":"adding_new_models/#","page":"Adding new Models","title":"Adding new Models","text":"It is not necessary for fit to provide dimension checks or to call clean! on the model; MLJ will carry out such checks.","category":"page"},{"location":"adding_new_models/#","page":"Adding new Models","title":"Adding new Models","text":"The method fit should never alter hyperparameter values. If the package is able to suggest better hyperparameters, as a byproduct of training, return these in the report field.","category":"page"},{"location":"adding_new_models/#","page":"Adding new Models","title":"Adding new Models","text":"One should test that actual fit-results have the type declared in the model mutable struct declaration. To help with this, MLJBase.fitresult_type(m) returns the declared type, for any supervised model (or model type) m.","category":"page"},{"location":"adding_new_models/#","page":"Adding new Models","title":"Adding new Models","text":"The verbosity level (0 for silent) is for passing to learning algorithm itself. A fit method wrapping such an algorithm should generally avoid doing any of its own logging.","category":"page"},{"location":"adding_new_models/#The-predict-method-1","page":"Adding new Models","title":"The predict method","text":"","category":"section"},{"location":"adding_new_models/#","page":"Adding new Models","title":"Adding new Models","text":"The compulsory predict method has the form","category":"page"},{"location":"adding_new_models/#","page":"Adding new Models","title":"Adding new Models","text":"MLJBase.predict(model::SomeSupervisedModelType, fitresult, Xnew) -> yhat","category":"page"},{"location":"adding_new_models/#","page":"Adding new Models","title":"Adding new Models","text":"Here Xnew is an arbitrary table (see above).","category":"page"},{"location":"adding_new_models/#","page":"Adding new Models","title":"Adding new Models","text":"Prediction types for deterministic responses. In the case of Deterministic models, yhat must have the same form as the target y passed to the fit method (see above discussion on the form of data for fitting), with one exception: If predicting a Count, the prediction may be Continuous. For all models predicting a Multiclass or FiniteOrderedFactor, the categorical vectors returned by predict must have the levels in the categorical pool of the target data presented in training, even if not all levels appear in the training data or prediction itself. That is, we must have levels(yhat) == levels(y).","category":"page"},{"location":"adding_new_models/#","page":"Adding new Models","title":"Adding new Models","text":"Unfortunately, code not written with the preservation of categorical levels in mind poses special problems. To help with this, MLJ provides a utility CategoricalDecoder which can decode a CategoricalArray into a plain array, and re-encode a prediction with the original levels intact. The CategoricalDecoder object created during fit will need to be bundled with fitresult to make it available to predict during re-encoding. (If you are coding a learning algorithm  from scratch, rather than  wrapping an existing one, conversions may be unnecessary. It may suffice  to record the pool of y and bundle that with the fitresult for predict to append  to the levels of its categorical output, or to add to the support of the predicted  probability distributions.)","category":"page"},{"location":"adding_new_models/#","page":"Adding new Models","title":"Adding new Models","text":"So, for example, if the core algorithm being wrapped by fit expects a nominal target yint of type Vector{Int64} then a fit method may look something like this:","category":"page"},{"location":"adding_new_models/#","page":"Adding new Models","title":"Adding new Models","text":"function MLJBase.fit(model::SomeSupervisedModelType, verbosity, X, y)\n    decoder = MLJBase.CategoricalDecoder(y, Int64)\n    yint = transform(decoder, y)\n    core_fitresult = SomePackage.fit(X, yint, verbosity=verbosity)\n    fitresult = (decoder, core_fitresult)\n    cache = nothing\n    report = nothing\n    return fitresult, cache, report\nend","category":"page"},{"location":"adding_new_models/#","page":"Adding new Models","title":"Adding new Models","text":"while a corresponding deterministic predict operation might look like this:","category":"page"},{"location":"adding_new_models/#","page":"Adding new Models","title":"Adding new Models","text":"function MLJBase.predict(model::SomeSupervisedModelType, fitresult, Xnew)\n    decoder, core_fitresult = fitresult\n    yhat = SomePackage.predict(core_fitresult, Xnew)\n    return inverse_transform(decoder, yhat)\nend","category":"page"},{"location":"adding_new_models/#","page":"Adding new Models","title":"Adding new Models","text":"Query ?MLJBase.DecodeCategorical for more information.","category":"page"},{"location":"adding_new_models/#","page":"Adding new Models","title":"Adding new Models","text":"Prediction types for probabilistic responses. In the case of Probabilistic models with univariate targets, yhat must be a Vector whose elements are distributions (one distribution per row of Xnew).","category":"page"},{"location":"adding_new_models/#","page":"Adding new Models","title":"Adding new Models","text":"A distribution is any instance of a subtype of Distributions.Distribution from the package Distributions.jl, or any instance of the additional types UnivariateNominal and MultivariateNominal defined in MLJBase.jl (or any other type D you define for which MLJBase.isdistribution(::D) = true, meaning Base.rand and Distributions.pdf are implemented, as well Distributions.mean/Distribution.median or Distributions.mode).","category":"page"},{"location":"adding_new_models/#","page":"Adding new Models","title":"Adding new Models","text":"Use UnivariateNominal for Probabilistic models predicting Multiclass or FiniteOrderedFactor targets. For example, suppose levels(y)=[\"yes\", \"no\", \"maybe\"] and set L=levels(y). Then, if the predicted probabilities for some input pattern are [0.1, 0.7, 0.2], respectively, then the prediction returned for that pattern will be UnivariateNominal(L, [0.1, 0.7, 0.2]). Query ?UnivariateNominal for more information.","category":"page"},{"location":"adding_new_models/#","page":"Adding new Models","title":"Adding new Models","text":"The predict method will need access to all levels in the pool of the target variable presented y presented for training, which consequently need to be encoded in the fitresult returned by fit. If a CategoricalDecoder object, decoder, has been bundled in fitresult, as in the deterministic example above, then the levels are given by levels(decoder). Levels not observed in the training data  (i.e., only in its pool) should be assigned probability zero.","category":"page"},{"location":"adding_new_models/#Trait-declarations-1","page":"Adding new Models","title":"Trait declarations","text":"","category":"section"},{"location":"adding_new_models/#","page":"Adding new Models","title":"Adding new Models","text":"There are a number of recommended trait declarations for each model mutable structure SomeSupervisedModel <: Supervised you define. Basic fitting, resampling and tuning in MLJ does not require these traits but some advanced MLJ meta-algorithms may require them now, or in the future. In particular, MLJ's models(::Task) method (matching models to user-specified tasks) can only identify models having a complete set of trait declarations. A full set of declarations is shown below for the DecisionTreeClassifier type (defined in the submodule DecisionTree_ of MLJModels):","category":"page"},{"location":"adding_new_models/#","page":"Adding new Models","title":"Adding new Models","text":"MLJBase.load_path(::Type{<:DecisionTreeClassifier}) = \"MLJModels.DecisionTree_.DecisionTreeClassifier\" \nMLJBase.package_name(::Type{<:DecisionTreeClassifier}) = \"DecisionTree\"\nMLJBase.package_uuid(::Type{<:DecisionTreeClassifier}) = \"7806a523-6efd-50cb-b5f6-3fa6f1930dbb\"\nMLJBase.package_url(::Type{<:DecisionTreeClassifier}) = \"https://github.com/bensadeghi/DecisionTree.jl\"\nMLJBase.is_pure_julia(::Type{<:DecisionTreeClassifier}) = true\nMLJBase.input_is_multivariate(::Type{<:DecisionTreeClassifier}) = true\nMLJBase.input_scitypes(::Type{<:DecisionTreeClassifier}) = MLJBase.Continuous\nMLJBase.target_scitype(::Type{<:DecisionTreeClassifier}) = MLJBase.Multiclass","category":"page"},{"location":"adding_new_models/#","page":"Adding new Models","title":"Adding new Models","text":"Note that models predicting multivariate targets will need to need to have target_scitype return an appropriate Tuple type. ","category":"page"},{"location":"adding_new_models/#","page":"Adding new Models","title":"Adding new Models","text":"For an explanation of Found and Other in the table below, see Scientific Types.","category":"page"},{"location":"adding_new_models/#","page":"Adding new Models","title":"Adding new Models","text":"method return type declarable return values default value\ntarget_scitype DataType subtype of Found or tuple of such types Union{Found,NTuple{<:Found}}\ninput_scitypes DataType subtype of Union{Missing,Found} Union{Missing,Found}\ninput_is_multivariate Bool true or false true\nis_pure_julia Bool true or false false\nload_path String unrestricted \"unknown\"\npackage_name String unrestricted \"unknown\"\npackage_uuid String unrestricted \"unknown\"\npackage_url String unrestricted \"unknown\"","category":"page"},{"location":"adding_new_models/#","page":"Adding new Models","title":"Adding new Models","text":"You can test declarations of traits by calling info(SomeModelType).","category":"page"},{"location":"adding_new_models/#The-update!-method-1","page":"Adding new Models","title":"The update! method","text":"","category":"section"},{"location":"adding_new_models/#","page":"Adding new Models","title":"Adding new Models","text":"An update method may be overloaded to enable a call by MLJ to retrain a model (on the same training data) to avoid repeating computations unnecessarily.","category":"page"},{"location":"adding_new_models/#","page":"Adding new Models","title":"Adding new Models","text":"MLJBase.update(model::SomeSupervisedModelType, verbosity, old_fitresult, old_cache, X, y) -> fitresult, cache, report","category":"page"},{"location":"adding_new_models/#","page":"Adding new Models","title":"Adding new Models","text":"If an MLJ Machine is being fit! and it is not the first time, then update is called instead of fit unless fit! has been called with new rows. However, MLJBase defines a fallback for update which just calls fit. For context, see MLJ Internals. Learning networks wrapped as models constitute one use-case: One would like each component model to be retrained only when hyperparameter changes \"upstream\" make this necessary. In this case MLJ provides a fallback (specifically, the fallback is for any subtype of Supervised{Node}). A second important use-case is iterative models, where calls to increase the number of iterations only restarts the iterative procedure if other hyperparameters have also changed. For an example, see builtins/Ensembles.jl.","category":"page"},{"location":"adding_new_models/#","page":"Adding new Models","title":"Adding new Models","text":"In the event that the argument fitresult (returned by a preceding call to fit) is not sufficient for performing an update, the author can arrange for fit to output in its cache return value any additional information required, as this is also passed as an argument to the update method.","category":"page"},{"location":"adding_new_models/#Multivariate-models-1","page":"Adding new Models","title":"Multivariate models","text":"","category":"section"},{"location":"adding_new_models/#","page":"Adding new Models","title":"Adding new Models","text":"TODO","category":"page"},{"location":"adding_new_models/#Unsupervised-models-1","page":"Adding new Models","title":"Unsupervised models","text":"","category":"section"},{"location":"adding_new_models/#","page":"Adding new Models","title":"Adding new Models","text":"TODO","category":"page"},{"location":"adding_new_models/#Where-to-place-code-implementing-new-models-1","page":"Adding new Models","title":"Where to place code implementing new models","text":"","category":"section"},{"location":"adding_new_models/#","page":"Adding new Models","title":"Adding new Models","text":"Note that different packages can implement models having the same name without causing conflicts, although an MLJ user cannot simultaneously load two such models.","category":"page"},{"location":"adding_new_models/#","page":"Adding new Models","title":"Adding new Models","text":"There are two options for making a new model available to all MLJ users:","category":"page"},{"location":"adding_new_models/#","page":"Adding new Models","title":"Adding new Models","text":"Native implementations (preferred option). The implementation code lives in the same package that contains the learning algorithms implementing the interface. In this case, it is sufficient to open an issue at MLJRegistry requesting the package to be registered with MLJ. Registering a package allows the MLJ user to access its models' metadata and to selectively load them.\nExternal implementations (short-term alternative). The model implementation code is necessarily separate from the package SomePkg defining the learning algorithm being wrapped. In this case, the recommended procedure is to include the implementation code at MLJModels/src via a pull-request, and test code at MLJModels/test. Assuming SomePkg is the only package imported by the implementation code, one needs to: (i) register SomePkg at MLJRegistry as explained above; and (ii) add a corresponding @require line in the PR to MLJModels/src/MLJModels.jl to enable lazy-loading of that package by MLJ (following the pattern of existing additions). If other packages must be imported, add them to the MLJModels project file after checking they are not already there. If it is really necessary, packages can be also added to Project.toml for testing purposes.","category":"page"},{"location":"adding_new_models/#","page":"Adding new Models","title":"Adding new Models","text":"Additionally, one needs to ensure that the implementation code defines the package_name and load_path model traits appropriately, so that MLJ's @load macro can find the necessary code (see MLJModels/src for examples). The @load command can only be tested after registration. If changes are made, lodge an issue at MLJRegistry to make the changes available to MLJ.  ","category":"page"},{"location":"#MLJ.jl-1","page":"Documentation","title":"MLJ.jl","text":"","category":"section"},{"location":"#","page":"Documentation","title":"Documentation","text":"Documentation for MLJ.jl","category":"page"},{"location":"#","page":"Documentation","title":"Documentation","text":"","category":"page"},{"location":"#Functions-1","page":"Documentation","title":"Functions","text":"","category":"section"},{"location":"#","page":"Documentation","title":"Documentation","text":"@curve(var1, range, code)","category":"page"},{"location":"#MLJ.@curve-Tuple{Any,Any,Any}","page":"Documentation","title":"MLJ.@curve","text":"@curve\n\nThe code,\n\n@curve var range code\n\nevaluates code, replacing appearances of var therein with each value in range. The range and corresponding evaluations are returned as a tuple of arrays. For example,\n\n@curve  x 1:3 (x^2 + 1)\n\nevaluates to\n\n([1,2,3], [2, 5, 10])\n\nThis is convenient for plotting functions using, eg, the Plots package:\n\nplot(@curve x 1:3 (x^2 + 1))\n\nA macro @pcurve parallelizes the same behaviour.  A two-variable implementation is also available, operating as in the following example:\n\njulia> @curve x [1,2,3] y [7,8] (x + y)\n([1,2,3],[7 8],[8.0 9.0; 9.0 10.0; 10.0 11.0])\n\njulia> ans[3]\n3×2 Array{Float64,2}:\n  8.0   9.0\n  9.0  10.0\n 10.0  11.0\n\nN.B. The second range is returned as a row vector for consistency with the output matrix. This is also helpful when plotting, as in:\n\njulia> u1, u2, A = @curve x range(0, stop=1, length=100) α [1,2,3] x^α\njulia> u2 = map(u2) do α \"α = \"*string(α) end\njulia> plot(u1, A, label=u2)\n\nwhich generates three superimposed plots - of the functions x, x^2 and x^3 - each labels with the exponents α = 1, 2, 3 in the legend.\n\n\n\n\n\n","category":"macro"},{"location":"#","page":"Documentation","title":"Documentation","text":"Modules = [MLJ,MLJBase,MLJModels]","category":"page"},{"location":"#MLJ.EnsembleModel-Tuple{}","page":"Documentation","title":"MLJ.EnsembleModel","text":"EnsembleModel(atom=nothing, weights=Float64[], bagging_fraction=0.8, rng_seed=0, n=100, parallel=true)\n\nCreate a model for training an ensemble of n learners, with optional bagging, each with associated model atom. Ensembling is useful if fit!(machine(atom, data...)) does not create identical models on repeated calls (ie, is a stochastic model, such as a decision tree with randomized node selection criteria), or if bagging_fraction is set to a value not equal to 1.0 (or both). The constructor fails if no atom is specified.\n\nPredictions are weighted according to the vector weights (to allow for external optimization) except in the case that atom is a Deterministic classifier. Uniform weights are used if weight has zero length.\n\nThe ensemble model is Deterministic or Probabilistic, according to the corresponding supertype of atom. In the case of classifiers (targetscitype(atom) <: Union{Multiclass,FiniteOrderedFactor}), the predictions are majority votes, and for regressors (targetscitype(atom)<: Continuous) they are ordinary averages. Probabilistic predictions are obtained by averaging the atomic probability distribution functions; in particular, for regressors, the ensemble prediction on each input pattern has the type MixtureModel{VF,VS,D} from the Distributions.jl package, where D is the type of predicted distribution for atom.\n\n\n\n\n\n","category":"method"},{"location":"#MLJ.evaluate!-Tuple{Machine}","page":"Documentation","title":"MLJ.evaluate!","text":"evaluate!(mach, resampling=CV(), measure=nothing, operation=predict, verbosity=1)\n\nEstimate the performance of a machine mach using the specified resampling (defaulting to 6-fold cross-validation) and measure. In general this mutating operation preserves only mach.args (the data stored in the machine).\n\nIf no measure is specified, then default_measure(mach.model) is used, unless this default is nothing and an error is thrown.\n\n\n\n\n\n","category":"method"},{"location":"#MLJ.iterator-Union{Tuple{M}, Tuple{M,Params}} where M<:Model","page":"Documentation","title":"MLJ.iterator","text":"iterator(model::Model, param_iterators::Params)\n\nIterator over all models of type typeof(model) defined by param_iterators.\n\nEach name in the nested :name => value pairs of param_iterators should be the name of a (possibly nested) field of model; and each element of flat_values(param_iterators) (the corresponding final values) is an iterator over values of one of those fields.\n\nSee also iterator and params.\n\n\n\n\n\n","category":"method"},{"location":"#MLJ.learning_curve-Tuple{Supervised,Any,Vararg{Any,N} where N}","page":"Documentation","title":"MLJ.learning_curve","text":"learning_curve(model, X, ys...; resolution=30, resampling=Holdout(), measure=rms, operation=pr, param_range=nothing)\n\nReturns (u, v) where u is a vector of hyperparameter values, and v the corresponding performance estimates. \n\nX, y = datanow()\natom = RidgeRegressor()\nmodel = EnsembleModel(atom=atom)\nr = range(atom, :lambda, lower=0.1, upper=100, scale=:log10)\nparam_range = Params(:atom => Params(:lambda => r))\nu, v = MLJ.learning_curve(model, X, y; param_range = param_range) \n\n\n\n\n\n","category":"method"},{"location":"#MLJ.models-Tuple{}","page":"Documentation","title":"MLJ.models","text":"models()\n\nList the names of all MLJ models, loaded or registered, as a dictionary indexed on package name.\n\n\n\n\n\n","category":"method"},{"location":"#MLJ.rmsp-Tuple{AbstractArray{#s12,1} where #s12<:Real,Any}","page":"Documentation","title":"MLJ.rmsp","text":"Root mean squared percentage loss \n\n\n\n\n\n","category":"method"},{"location":"#MLJ.strange-Tuple{Model,Symbol}","page":"Documentation","title":"MLJ.strange","text":"strange(model, :hyper; kwargs...)\n\nReturns the pair :hyper => range(model, :hyper; kwargs...); \"strange\" is short for \"set to range\".\n\nSee also: range\n\n\n\n\n\n","category":"method"},{"location":"#MLJBase.info","page":"Documentation","title":"MLJBase.info","text":"info(model, pkg=nothing)\n\nReturn the dictionary of metadata associated with model::String. If more than one package implements model then pkg::String will need to be specified.\n\n\n\n\n\n","category":"function"},{"location":"#MLJ.SCALE","page":"Documentation","title":"MLJ.SCALE","text":"Scale = SCALE()\n\nObject for dispatching on scales and functions when generating parameter ranges. We require different behaviour for scales and functions:\n\n transform(Scale, scale(:log), 100) = 2\n inverse_transform(Scale, scale(:log), 2) = 100\n\nbut     transform(Scale, scale(log), 100) = 100       # identity     inverse_transform(Scale, scale(log), 100) = 2 \n\nSee also: strange\n\n\n\n\n\n","category":"type"},{"location":"#MLJ.SimpleDeterministicCompositeModel","page":"Documentation","title":"MLJ.SimpleDeterministicCompositeModel","text":"SimpleDeterministicCompositeModel(;regressor=ConstantRegressor(), \n                          transformer=FeatureSelector())\n\nConstruct a composite model consisting of a transformer (Unsupervised model) followed by a Deterministic model. Mainly intended for internal testing .\n\n\n\n\n\n","category":"type"},{"location":"#Base.copy","page":"Documentation","title":"Base.copy","text":"copy(params::Params, values=nothing)\n\nReturn a copy of params with new values. That is, flat_values(copy(params, values)) == values is true, while the first element of each nested pair (parameter name) is unchanged.\n\nIf values is not specified a deep copy is returned. \n\n\n\n\n\n","category":"function"},{"location":"#Base.merge!-Tuple{Array{T,1} where T,Array{T,1} where T}","page":"Documentation","title":"Base.merge!","text":"merge!(tape1, tape2)\n\nIncrementally appends to tape1 all elements in tape2, excluding any element previously added (or any element of tape1 in its initial state).\n\n\n\n\n\n","category":"method"},{"location":"#Base.range-Union{Tuple{D}, Tuple{MLJType,Symbol}} where D","page":"Documentation","title":"Base.range","text":"r = range(model, :hyper; values=nothing)\n\nDefines a NominalRange object for a field hyper of model. Note that r is not directly iterable but iterator(r) iterates over values.\n\nr = range(model, :hyper; upper=nothing, lower=nothing, scale=:linear)\n\nDefines a NumericRange object for a field hyper of model.  Note that r is not directly iteratable but iterator(r, n) iterates over n values between lower and upper values, according to the specified scale. The supported scales are :linear, :log, :log10, :log2. Values for Integer types are rounded (with duplicate values removed, resulting in possibly less than n values).\n\nAlternatively, if a function f is provided as scale, then iterator(r, n) iterates over the values [f(x1), f(x2), ... , f(xn)], where x1, x2, ..., xn are linearly spaced between lower and upper.\n\nSee also: strange, iterator\n\n\n\n\n\n","category":"method"},{"location":"#MLJ.flat_keys-Tuple{Pair{Symbol,B} where B}","page":"Documentation","title":"MLJ.flat_keys","text":" flat_keys(params::Params)\n\nUse dot-concatentation to express each key in key-value pair of params in string form.\n\nExample\n\njulia> flat_keys(Params(:A => Params(:x => 2, :y => 3), :B)))\n[\"A.x\", \"A.y\", \"B\"]\n\n\n\n\n\n","category":"method"},{"location":"#MLJ.get_type-Tuple{Any,Symbol}","page":"Documentation","title":"MLJ.get_type","text":"get_type(T, field::Symbol)\n\nReturns the type of the field field of DataType T. Not a type-stable function.  \n\n\n\n\n\n","category":"method"},{"location":"#MLJ.unwind-Tuple","page":"Documentation","title":"MLJ.unwind","text":"unwind(iterators...)\n\nRepresent all possible combinations of values generated by iterators as rows of a matrix A. In more detail, A has one column for each iterator in iterators and one row for each distinct possible combination of values taken on by the iterators. Elements in the first column cycle fastest, those in the last clolumn slowest. \n\nExample\n\njulia> iterators = ([1, 2], [\"a\",\"b\"], [\"x\", \"y\", \"z\"]);\njulia> MLJ.unwind(iterators...)\n12×3 Array{Any,2}:\n 1  \"a\"  \"x\"\n 2  \"a\"  \"x\"\n 1  \"b\"  \"x\"\n 2  \"b\"  \"x\"\n 1  \"a\"  \"y\"\n 2  \"a\"  \"y\"\n 1  \"b\"  \"y\"\n 2  \"b\"  \"y\"\n 1  \"a\"  \"z\"\n 2  \"a\"  \"z\"\n 1  \"b\"  \"z\"\n 2  \"b\"  \"z\"\n\n\n\n\n\n","category":"method"},{"location":"#MLJBase.SupervisedTask-Tuple{}","page":"Documentation","title":"MLJBase.SupervisedTask","text":"task = SupervisedTask(; data=nothing, targets=nothing, ignore=Symbol[], probabilistic=nothing)\n\nConstruct a supervised learning task using data, which is any table supported by Tables.jl. Note that rows must correspond to patterns and columns to features. The name of the target column(s) is specfied by target (a symbol or vector of symbols). The input features are all the remaining columns of data, with the exception of those specified by ignore. All keyword arguments except ignore are compulsory.\n\nX, y = task()\n\nRetrieve the table of input features X and target(s) y. For single targets, y is a Vector or CategoricalVector. In the multivariate case, y is a named tuple of vectors (and, in particular, a table supported by Tables.jl). Additional methods give access to select parts of the data: \n\nX_(task), y_(task) == task()   # true\n\n\n\n\n\n","category":"method"},{"location":"#MLJBase.UnivariateNominal","page":"Documentation","title":"MLJBase.UnivariateNominal","text":"UnivariateNominal(prob_given_level)\n\nA discrete univariate distribution whose finite support is the set of keys of the provided dictionary, prob_given_level. The dictionary values specify the corresponding probabilities, which must be nonnegative and sum to one.\n\nUnivariateNominal(levels, p)\n\nA discrete univariate distribution whose finite support is the elements of the vector levels, and whose corresponding probabilities are elements of the vector p.\n\nlevels(d::UnivariateNominal)\n\nReturn the levels of d.\n\nd = UnivariateNominal([\"yes\", \"no\", \"maybe\"], [0.1, 0.2, 0.7])\npdf(d, \"no\") # 0.2\nmode(d) # \"maybe\"\nrand(d, 5) # [\"maybe\", \"no\", \"maybe\", \"maybe\", \"no\"]\nd = fit(UnivariateNominal, [\"maybe\", \"no\", \"maybe\", \"yes\"])\npdf(d, \"maybe\") ≈ 0.5 # true\nlevels(d) # [\"yes\", \"no\", \"maybe\"]\n\n\n\n\n\n","category":"type"},{"location":"#MLJBase.UnsupervisedTask-Tuple{}","page":"Documentation","title":"MLJBase.UnsupervisedTask","text":"task = UnsupervisedTask(; data=nothing, ignore=Symbol[])\n\nConstruct a unsupervised learning task using data, which is any table supported by Tables.jl. Note that rows must correspond to patterns and columns to features.  The input features for the task are all the columns of data, with the exception of those specified by ignore. \n\nX = task()\n\nRetrieve the table of inputs for task (the table data without features in ignore.\n\n\n\n\n\n","category":"method"},{"location":"#MLJBase.datanow-Tuple{}","page":"Documentation","title":"MLJBase.datanow","text":"Get some supervised data now!!\n\n\n\n\n\n","category":"method"},{"location":"#MLJBase.fitresult_type-Tuple{Type{#s35} where #s35<:Supervised}","page":"Documentation","title":"MLJBase.fitresult_type","text":"MLJBase.fitresult_type(m)\n\nReturns the fitresult type of any supervised model (or model type) m, as declared in the model mutable struct declaration.\n\n\n\n\n\n","category":"method"},{"location":"#MLJBase.load_ames-Tuple{}","page":"Documentation","title":"MLJBase.load_ames","text":"Load the full version of the well-known Ames Housing task.\n\n\n\n\n\n","category":"method"},{"location":"#MLJBase.load_boston-Tuple{}","page":"Documentation","title":"MLJBase.load_boston","text":"Load a well-known public regression dataset with nominal features.\n\n\n\n\n\n","category":"method"},{"location":"#MLJBase.load_crabs-Tuple{}","page":"Documentation","title":"MLJBase.load_crabs","text":"Load a well-known crab classification dataset with nominal features.\n\n\n\n\n\n","category":"method"},{"location":"#MLJBase.load_iris-Tuple{}","page":"Documentation","title":"MLJBase.load_iris","text":"Load a well-known public classification task with nominal features.\n\n\n\n\n\n","category":"method"},{"location":"#MLJBase.load_reduced_ames-Tuple{}","page":"Documentation","title":"MLJBase.load_reduced_ames","text":"Load a reduced version of the well-known Ames Housing task, having six numerical and six categorical features.\n\n\n\n\n\n","category":"method"},{"location":"#MLJBase.partition-Tuple{AbstractArray{Int64,1},Vararg{Any,N} where N}","page":"Documentation","title":"MLJBase.partition","text":"partition(rows::AbstractVector{Int}, fractions...; shuffle=false)\n\nSplits the vector rows into a tuple of vectors whose lengths are given by the corresponding fractions of length(rows). The last fraction is not provided, as it is inferred from the preceding ones. So, for example,\n\njulia> partition(1:1000, 0.2, 0.7)\n(1:200, 201:900, 901:1000)\n\n\n\n\n\n","category":"method"},{"location":"#MLJBase.schema-Tuple{Any}","page":"Documentation","title":"MLJBase.schema","text":"schema(X)\n\nReturns a struct with properties names, eltypes, nrows,ncols, with the obvious meanings. HereXis any object for whichTables.istable(X)` is true, an abstract matrix, or vector. \n\n\n\n\n\n","category":"method"},{"location":"#MLJBase.selectcols-Tuple{Any,Any}","page":"Documentation","title":"MLJBase.selectcols","text":"selectcols(X, c)\n\nSelect single or multiple columns from any object X for which Tables.istable(X) is true. If c is an abstract vector of integers or symbols, then the object returned is a table of the preferred sink type of typeof(X). If c is a single integer or column, then a Vector or CategoricalVector is returned. \n\nThe method is overloaded to additionally work on abstract matrices.\n\n\n\n\n\n","category":"method"},{"location":"#MLJBase.selectrows-Tuple{Any,Any}","page":"Documentation","title":"MLJBase.selectrows","text":"selectrows(X, r)\n\nSelect single or multiple rows from any object X for which Tables.istable(X) is true.  The object returned is a table of the preferred sink type of typeof(X), even a single row is selected.\n\nThe method is overloaded to additionally work on abstract matrices and vectors.\n\n\n\n\n\n","category":"method"},{"location":"#MLJBase.table-Tuple{NamedTuple}","page":"Documentation","title":"MLJBase.table","text":"MLJBase.table(cols; prototype=cols)\n\nConvert a named tuple of vectors cols, into a table. The table type returned is the \"preferred sink type\" for prototype (see the Tables.jl documentation). \n\nMLJBase.table(X::AbstractMatrix; names=nothing, prototype=nothing)\n\nConvert an abstract matrix X into a table with names (a tuple of symbols) as column names, or with labels (:x1, :x2, ..., :xn) where n=size(X, 2), if names is not specified.  If prototype=nothing, then a named tuple of vectors is returned.\n\nEquivalent to table(cols, prototype=prototype) where cols is the named tuple of columns of X, with keys(cols) = names.\n\n\n\n\n\n","category":"method"},{"location":"#MLJBase.@constant-Tuple{Any}","page":"Documentation","title":"MLJBase.@constant","text":"@constant x = value\n\nEquivalent to const x = value but registers the binding thus:\n\nMLJBase.HANDLE_GIVEN_ID[objectid(value)] = :x\n\nRegistered objects get displayed using the variable name to which it was bound in calls to show(x), etc. \n\nWARNING: As with any const declaration, binding x to new value of the same type is not prevented and the registration will not be updated.\n\n\n\n\n\n","category":"macro"},{"location":"#MLJBase.@more-Tuple{}","page":"Documentation","title":"MLJBase.@more","text":"@more\n\nEntered at the REPL, equivalent to show(ans, 100). Use to get a recursive description of all fields of the last REPL value.\n\n\n\n\n\n","category":"macro"},{"location":"#MLJBase.CategoricalDecoder","page":"Documentation","title":"MLJBase.CategoricalDecoder","text":"CategoricalDecoder(C::CategoricalArray)\nCategoricalDecoder(C::CategoricalArray, eltype)\n\nConstruct a decoder for transforming a CategoricalArray{T} object into an ordinary array, and for re-encoding similar arrays back into a CategoricalArray{T} object having the same pool (and, in particular, the same levels) as C. If eltype is not specified then the element type of the transformed array is T. Otherwise, the element type is eltype and the elements are promotions of the internal (integer) refs of the CategoricalArray. One must have R <: eltype <: Real where R is the reference type of the CategoricalArray (usually UInt32).\n\ntransform(decoder::CategoricalDecoder, C::CategoricalArray)\n\nTransform C into an ordinary Array.\n\ninverse_transform(decoder::CategoricalDecoder, A::Array)\n\nTransform an array A suitably compatible with decoder into a CategoricalArray having the same pool as C.\n\nlevels(decoder::CategoricalDecoder)\nlevels_seen(decoder::CategoricaDecoder)\n\nReturn, respectively, all levels in pool of the categorical vector C used to construct decoder (ie, levels(C)), and just those levels explicitly appearing as entries of C (ie, unique(C)).\n\nExample\n\n```` julia> using CategoricalArrays julia> C = categorical([\"a\" \"b\"; \"a\" \"c\"]) 2×2 CategoricalArray{String,2,UInt32}:  \"a\"  \"b\"  \"a\"  \"c\"\n\njulia> decoder = MLJBase.CategoricalDecoder(C, eltype=Float64); julia> A = transform(decoder, C) 2×2 Array{Float64,2}:  1.0  2.0  1.0  3.0\n\njulia> inverse_transform(decoder, A[1:1,:]) 1×2 CategoricalArray{String,2,UInt32}:  \"a\"  \"b\"\n\njulia> levels(ans) 3-element Array{String,1}:  \"a\"  \"b\"  \"c\"\n\n\n\n\n\n","category":"type"},{"location":"#Base.names-Tuple{SupervisedTask}","page":"Documentation","title":"Base.names","text":"names(task::SupervisedTask)\n\nReturn the names of all columns of the task data, excluding target columns and any names in task.ignore.\n\n\n\n\n\n","category":"method"},{"location":"#Base.names-Tuple{UnsupervisedTask}","page":"Documentation","title":"Base.names","text":"names(task::UnsupervisedTask)\n\nReturn the names of all columns of the task data, excluding any in task.ignore.\n\n\n\n\n\n","category":"method"},{"location":"#MLJBase._cummulative-Union{Tuple{UnivariateNominal{L,T}}, Tuple{T}, Tuple{L}} where T<:Real where L","page":"Documentation","title":"MLJBase._cummulative","text":"_cummulative(d::UnivariateNominal)\n\nReturn the cummulative probability vector [0, ..., 1] for the distribution d, using whatever ordering is used in the dictionary d.prob_given_level. Used only for to implement random sampling from d.\n\n\n\n\n\n","category":"method"},{"location":"#MLJBase._rand-Tuple{Any}","page":"Documentation","title":"MLJBase._rand","text":"rand(pcummulative)\n\nRandomly sample the distribution with discrete support 1:n which has cummulative probability vector p_cummulative=[0, ..., 1] (of length n+1). Does not check the first and last elements of p_cummulative but does not use them either. \n\n\n\n\n\n","category":"method"},{"location":"#MLJBase._recursive_show-Tuple{IO,MLJType,Any,Any}","page":"Documentation","title":"MLJBase._recursive_show","text":"_recursive_show(stream, object, current_depth, depth)\n\nGenerate a table of the field values of the MLJType object, dislaying each value by calling the method _show on it. The behaviour of _show(stream, f) is as follows:\n\nIf f is itself a MLJType object, then its short form is shown\n\nand _recursive_show generates as separate table for each of its field values (and so on, up to a depth of argument depth).\n\nOtherwise f is displayed as \"(omitted T)\" where T = typeof(f),\n\nunless istoobig(f) is false (the istoobig fall-back for arbitrary types being true). In the latter case, the long (ie, MIME\"plain/text\") form of f is shown. To override this behaviour, overload the _show method for the type in question. \n\n\n\n\n\n","category":"method"},{"location":"#MLJBase.abbreviated-Tuple{Any}","page":"Documentation","title":"MLJBase.abbreviated","text":"to display abbreviated versions of integers\n\n\n\n\n\n","category":"method"},{"location":"#MLJBase.handle-Tuple{Any}","page":"Documentation","title":"MLJBase.handle","text":"return abbreviated object id (as string)  or it's registered handle (as string) if this exists\n\n\n\n\n\n","category":"method"},{"location":"#MLJBase.matrix-Tuple{Any}","page":"Documentation","title":"MLJBase.matrix","text":"\"     MLJBase.matrix(X)\n\nConvert a generic table source X into an Matrix; or, if X is a AbstractMatrix, return X. Optimized for column-based sources.\n\n\n\n\n\n","category":"method"},{"location":"#Index-1","page":"Documentation","title":"Index","text":"","category":"section"},{"location":"#","page":"Documentation","title":"Documentation","text":"","category":"page"}]
}
