var documenterSearchIndex = {"docs":
[{"location":"adding_new_models/#Implementing-the-MLJ-interface-for-a-learning-algorithm-1","page":"Implementing the MLJ interface for a learning algorithm","title":"Implementing the MLJ interface for a learning algorithm","text":"","category":"section"},{"location":"adding_new_models/#","page":"Implementing the MLJ interface for a learning algorithm","title":"Implementing the MLJ interface for a learning algorithm","text":"This guide outlines the specification of the MLJ model interface. The machine learning tools provided by MLJ can be applied to the models in any package that imports the module MLJBase and implements the API defined there as outlined below.","category":"page"},{"location":"adding_new_models/#","page":"Implementing the MLJ interface for a learning algorithm","title":"Implementing the MLJ interface for a learning algorithm","text":"To implement the API described here, some familiarity with the following packages is helpful: Distributions.jl (for probabilistic predictions), CategoricalArrays.jl (essential if you are implementing a classifier, or a learner that handles categorical inputs), Tables.jl (if you're algorithm needs input data in a novel format).","category":"page"},{"location":"adding_new_models/#","page":"Implementing the MLJ interface for a learning algorithm","title":"Implementing the MLJ interface for a learning algorithm","text":"For a quick and dirty implementation of user-defined models see here.","category":"page"},{"location":"adding_new_models/#","page":"Implementing the MLJ interface for a learning algorithm","title":"Implementing the MLJ interface for a learning algorithm","text":"<!– As a temporary measure, –> <!– the MLJ package also implements the MLJ interface for some –> <!– non-compliant packages, using lazily loaded modules (\"glue code\") –> <!– residing in –> <!– src/interfaces –> <!– of the MLJ.jl repository. A checklist for adding models in this latter –> <!– way is given at the end; a template is given here: –> <!– \"src/interfaces/DecisionTree.jl\". –>","category":"page"},{"location":"adding_new_models/#","page":"Implementing the MLJ interface for a learning algorithm","title":"Implementing the MLJ interface for a learning algorithm","text":"In MLJ, the basic interface exposed to the user, built atop the model interface described here, is the machine interface. After a first reading of this document, the reader may wish to refer to the simplified description of the machine interface appearing under \"MLJ Internals\".","category":"page"},{"location":"adding_new_models/#","page":"Implementing the MLJ interface for a learning algorithm","title":"Implementing the MLJ interface for a learning algorithm","text":"<!– ### MLJ types –>","category":"page"},{"location":"adding_new_models/#","page":"Implementing the MLJ interface for a learning algorithm","title":"Implementing the MLJ interface for a learning algorithm","text":"<!– Every type introduced the core MLJ package should be a subtype of: –>","category":"page"},{"location":"adding_new_models/#","page":"Implementing the MLJ interface for a learning algorithm","title":"Implementing the MLJ interface for a learning algorithm","text":"<!– --> <!-- abstract type MLJType end --> <!-- –>","category":"page"},{"location":"adding_new_models/#","page":"Implementing the MLJ interface for a learning algorithm","title":"Implementing the MLJ interface for a learning algorithm","text":"<!– The Julia show method is informatively overloaded for this –> <!– type. Variable bindings declared with @constant \"register\" the –> <!– binding, which is reflected in the output of show. –>","category":"page"},{"location":"adding_new_models/#Overview-1","page":"Implementing the MLJ interface for a learning algorithm","title":"Overview","text":"","category":"section"},{"location":"adding_new_models/#","page":"Implementing the MLJ interface for a learning algorithm","title":"Implementing the MLJ interface for a learning algorithm","text":"A model is an object storing hyperparameters associated with some machine learning algorithm, where \"learning algorithm\" is broadly interpreted.  In MLJ, hyperparameters include configuration parameters, like the number of threads, and special instructions, such as \"compute feature rankings\", which may or may not affect the final learning outcome.  However, the logging level (verbosity below) is excluded.","category":"page"},{"location":"adding_new_models/#","page":"Implementing the MLJ interface for a learning algorithm","title":"Implementing the MLJ interface for a learning algorithm","text":"The name of the Julia type associated with a model indicates the associated algorithm (e.g., DecisionTreeClassifier). The outcome of training a learning algorithm is here called a fit-result. For ordinary multilinear regression, for example, this would be the coefficients and intercept. For a general supervised model, it is the (generally minimal) information needed to make new predictions.","category":"page"},{"location":"adding_new_models/#","page":"Implementing the MLJ interface for a learning algorithm","title":"Implementing the MLJ interface for a learning algorithm","text":"The ultimate supertype of all models is MLJBase.Model, which has two abstract subtypes; quoting MLJBase.jl:","category":"page"},{"location":"adding_new_models/#","page":"Implementing the MLJ interface for a learning algorithm","title":"Implementing the MLJ interface for a learning algorithm","text":"abstract type MLJType end\nabstract type Supervised{R} <: Model end\nabstract type Unsupervised <: Model end","category":"page"},{"location":"adding_new_models/#","page":"Implementing the MLJ interface for a learning algorithm","title":"Implementing the MLJ interface for a learning algorithm","text":"Here the parameter R refers to a fit-result type. By declaring a model to be a subtype of MLJBase.Supervised{R} you guarantee the fit-result to be of type R and, if R is concrete, one may improve the performance of homogeneous ensembles of the model (as defined by the built-in MLJ EnsembleModel wrapper). There is no abstract type for fit-results because these types are generally declared outside of MLJBase.","category":"page"},{"location":"adding_new_models/#","page":"Implementing the MLJ interface for a learning algorithm","title":"Implementing the MLJ interface for a learning algorithm","text":"Supervised models are further divided according to whether they are able to furnish probabilistic predictions of the target(s) (which they will do so by default) or directly predict \"point\" estimates, for each new input pattern:","category":"page"},{"location":"adding_new_models/#","page":"Implementing the MLJ interface for a learning algorithm","title":"Implementing the MLJ interface for a learning algorithm","text":"abstract type Probabilistic{R} <: Supervised{R} end\nabstract type Deterministic{R} <: Supervised{R} end","category":"page"},{"location":"adding_new_models/#","page":"Implementing the MLJ interface for a learning algorithm","title":"Implementing the MLJ interface for a learning algorithm","text":"Further division of model types is realized through \"trait\" declarations (see below). ","category":"page"},{"location":"adding_new_models/#","page":"Implementing the MLJ interface for a learning algorithm","title":"Implementing the MLJ interface for a learning algorithm","text":"Associated with every concrete subtype of Model there must be a fit method, which implements the associated algorithm to produce the fit-result. Additionally, every Supervised model has a predict method, while Unsupervised models must have a transform method. More generally, methods such as these, that are dispatched on a model instance and a fit-result (plus other data), are called operations. Probabilistic supervised models optionally implement a predict_mode operation (in the case of classifiers) or a predict_mean and/or predict_median operations (in the case of regressors) overriding obvious fallbacks provided by MLJBase. Unsupervised models may implement an inverse_transform operation.","category":"page"},{"location":"adding_new_models/#The-Model-API-1","page":"Implementing the MLJ interface for a learning algorithm","title":"The Model API","text":"","category":"section"},{"location":"adding_new_models/#","page":"Implementing the MLJ interface for a learning algorithm","title":"Implementing the MLJ interface for a learning algorithm","text":"<!– Every package interface should live inside a submodule for namespace –> <!– hygiene (see the template at –> <!– \"src/interfaces/DecisionTree.jl\"). Ideally, package interfaces should –> <!– export no struct outside of the new model types they define, and –> <!– import only abstract types. All \"structural\" design should be –> <!– restricted to the MLJ core to prevent rewriting glue code when there –> <!– are design changes. –>","category":"page"},{"location":"adding_new_models/#New-model-type-declarations-and-optional-clean!-method-1","page":"Implementing the MLJ interface for a learning algorithm","title":"New model type declarations and optional clean! method","text":"","category":"section"},{"location":"adding_new_models/#","page":"Implementing the MLJ interface for a learning algorithm","title":"Implementing the MLJ interface for a learning algorithm","text":"Here is an example of a concrete supervised model type declaration, made after defining an appropriate fit-result type (an optional step):","category":"page"},{"location":"adding_new_models/#","page":"Implementing the MLJ interface for a learning algorithm","title":"Implementing the MLJ interface for a learning algorithm","text":"import MLJ\n\nstruct LinearFitResult{F<:AbstractFloat} <: MLJBase.MLJType\n    coefficients::Vector{F}\n    bias::F\nend\n\nmutable struct RidgeRegressor{F} <: MLJBase.Deterministic{LinearFitResult{F}}\n    target_type::Type{F}\n    lambda::Float64\nend","category":"page"},{"location":"adding_new_models/#","page":"Implementing the MLJ interface for a learning algorithm","title":"Implementing the MLJ interface for a learning algorithm","text":"Models (which are mutable) should not be given internal constructors. It is recommended that they be given an external lazy keyword constructor of the same name. This constructor defines default values for every field, and optionally corrects invalid field values by calling a clean! method (whose fallback returns an empty message string):","category":"page"},{"location":"adding_new_models/#","page":"Implementing the MLJ interface for a learning algorithm","title":"Implementing the MLJ interface for a learning algorithm","text":"function MLJ.clean!(model::RidgeRegressor)\n    warning = \"\"\n    if model.lambda < 0\n        warning *= \"Need lambda ≥ 0. Resetting lambda=0. \"\n        model.lambda = 0\n    end\n    return warning\nend\n\n# keyword constructor\nfunction RidgeRegressor(; target_type=Float64, lambda=0.0)\n\n    model = RidgeRegressor(target_type, lambda)\n\n    message = MLJBase.clean!(model)\n    isempty(message) || @warn message\n\n    return model\n    \nend","category":"page"},{"location":"adding_new_models/#Supervised-models-1","page":"Implementing the MLJ interface for a learning algorithm","title":"Supervised models","text":"","category":"section"},{"location":"adding_new_models/#","page":"Implementing the MLJ interface for a learning algorithm","title":"Implementing the MLJ interface for a learning algorithm","text":"Below we describe the compulsory and optional methods to be specified for each concrete type SomeSupervisedModel{R} <: MLJBase.Supervised{R}. We restrict attention to algorithms handling a single (univariate) target. Differences in the multivariate case are described later.","category":"page"},{"location":"adding_new_models/#The-form-of-data-for-fitting-and-prediction,-and-the-ouput_kind-trait-1","page":"Implementing the MLJ interface for a learning algorithm","title":"The form of data for fitting and prediction, and the ouput_kind trait","text":"","category":"section"},{"location":"adding_new_models/#","page":"Implementing the MLJ interface for a learning algorithm","title":"Implementing the MLJ interface for a learning algorithm","text":"The argument X passed to the fit method described below, and the argument Xnew of the predict (or transform) method, are arbitrary tables. Here table means an object supporting theTables.jl interface. If the core algorithm requires data in a different or more specific form, then fit will need to coerce the table into the form desired. To this end, MLJ provides the convenience method MLJBase.matrix; MLJBase.matrix(Xtable) is a two-dimensional Array{T} where T is the tightest common type of elements of Xtable, and Xtable is any table. ","category":"page"},{"location":"adding_new_models/#","page":"Implementing the MLJ interface for a learning algorithm","title":"Implementing the MLJ interface for a learning algorithm","text":"Other convenience methods provided by MLJBase for handling tabular data are: selectrows, selectcols, schema (for extracting the size, names and eltypes of a table) and table (for materializing an abstract matrix, or named tuple of vectors, as a table matching a given prototype). Query the doc-strings for details.","category":"page"},{"location":"adding_new_models/#","page":"Implementing the MLJ interface for a learning algorithm","title":"Implementing the MLJ interface for a learning algorithm","text":"Note that generally the same type coercions applied to X by fit will need to be applied by predict to Xnew. ","category":"page"},{"location":"adding_new_models/#","page":"Implementing the MLJ interface for a learning algorithm","title":"Implementing the MLJ interface for a learning algorithm","text":"Important convention It is to be understood that the columns of the table X correspond to features and the rows to patterns.","category":"page"},{"location":"adding_new_models/#","page":"Implementing the MLJ interface for a learning algorithm","title":"Implementing the MLJ interface for a learning algorithm","text":"The form of the target data y passed to fit depends on the kind of supervised model. For \"regressors\", y is a vector, for \"classifiers\" it must be a categorical vector. More precisely, the form is determined by value returned by the trait output_kind that each model type should define (see below):","category":"page"},{"location":"adding_new_models/#","page":"Implementing the MLJ interface for a learning algorithm","title":"Implementing the MLJ interface for a learning algorithm","text":"output_kind return value type of y\n:continuous Vector{<:AbstractFloat}\n:binary CategoricalVector\n:multiclass CategoricalVector\n:ordered_factor_finite CategoricalVector\n:ordered_factor_infinite Vector{<:Integer}","category":"page"},{"location":"adding_new_models/#The-fit-method-1","page":"Implementing the MLJ interface for a learning algorithm","title":"The fit method","text":"","category":"section"},{"location":"adding_new_models/#","page":"Implementing the MLJ interface for a learning algorithm","title":"Implementing the MLJ interface for a learning algorithm","text":"A compulsory fit method returns three objects:","category":"page"},{"location":"adding_new_models/#","page":"Implementing the MLJ interface for a learning algorithm","title":"Implementing the MLJ interface for a learning algorithm","text":"MLJBase.fit(model::SomeSupervisedModelType, verbosity::Int, X, y) -> fitresult, cache, report","category":"page"},{"location":"adding_new_models/#","page":"Implementing the MLJ interface for a learning algorithm","title":"Implementing the MLJ interface for a learning algorithm","text":"Note: The Int typing of verbosity cannot be omitted.","category":"page"},{"location":"adding_new_models/#","page":"Implementing the MLJ interface for a learning algorithm","title":"Implementing the MLJ interface for a learning algorithm","text":"fitresult::R is the fit-result in the sense above (which becomes an  argument for predict discussed below).\nreport is either a Dict{Symbol,Any} object, or nothing if  there is nothing to report. So for example, fit might declare  report[:feature_importances] = ....  Any training-related  statistics, such as internal estimates of the generalization  error, feature rankings, and coefficients in linear models, should  be returned in the report dictionary. How, or if, these are  generated should be controlled by hyperparameters (the fields of  model). Reports get merged with those generated by previous  calls to fit by MLJ.","category":"page"},{"location":"adding_new_models/#","page":"Implementing the MLJ interface for a learning algorithm","title":"Implementing the MLJ interface for a learning algorithm","text":"3.\tThe value of cache can be nothing, unless one is also defining an     update method (see below). The Julia type of cache is not presently restricted.","category":"page"},{"location":"adding_new_models/#","page":"Implementing the MLJ interface for a learning algorithm","title":"Implementing the MLJ interface for a learning algorithm","text":"It is not necessary for fit to provide dimension checks or to call clean! on the model; MLJ will carry out such checks.","category":"page"},{"location":"adding_new_models/#","page":"Implementing the MLJ interface for a learning algorithm","title":"Implementing the MLJ interface for a learning algorithm","text":"The method fit should never alter hyperparameter values. If the package is able to suggest better hyperparameters, as a byproduct of training, return these in the report field.","category":"page"},{"location":"adding_new_models/#","page":"Implementing the MLJ interface for a learning algorithm","title":"Implementing the MLJ interface for a learning algorithm","text":"One should test that actual fit-results have the type declared in the model mutable struct declaration. To help with this, MLJBase.fitresult_type(m) returns the declared type, for any supervised model (or model type) m.","category":"page"},{"location":"adding_new_models/#","page":"Implementing the MLJ interface for a learning algorithm","title":"Implementing the MLJ interface for a learning algorithm","text":"The verbosity level (0 for silent) is for passing to learning algorithm itself. A fit method wrapping such an algorithm should generally avoid doing any of its own logging.","category":"page"},{"location":"adding_new_models/#The-predict-method-1","page":"Implementing the MLJ interface for a learning algorithm","title":"The predict method","text":"","category":"section"},{"location":"adding_new_models/#","page":"Implementing the MLJ interface for a learning algorithm","title":"Implementing the MLJ interface for a learning algorithm","text":"The compulsory predict method has the form","category":"page"},{"location":"adding_new_models/#","page":"Implementing the MLJ interface for a learning algorithm","title":"Implementing the MLJ interface for a learning algorithm","text":"MLJBase.predict(model::SomeSupervisedModelType, fitresult, Xnew) -> yhat","category":"page"},{"location":"adding_new_models/#","page":"Implementing the MLJ interface for a learning algorithm","title":"Implementing the MLJ interface for a learning algorithm","text":"Here Xnew is an arbitrary table (see above).","category":"page"},{"location":"adding_new_models/#","page":"Implementing the MLJ interface for a learning algorithm","title":"Implementing the MLJ interface for a learning algorithm","text":"Prediction types for deterministic responses. In the case of Deterministic models, yhat must have the same type as the target y passed to the fit method (see above discussion on the form of data for fitting), with one exception: If predicting an infinite ordered factor (where fit receives a Vector{<:Integer} object) the prediction may be continuous, i.e., of type Vector{<:AbstractFloat}. For all other classifiers, the categorical vector returned by predict must have the levels in the categorical pool of the target data presented in training, even if not all levels appear in the training data or prediction itself. That is, we must have levels(yhat) == levels(y).","category":"page"},{"location":"adding_new_models/#","page":"Implementing the MLJ interface for a learning algorithm","title":"Implementing the MLJ interface for a learning algorithm","text":"Unfortunately, code not written with the preservation of categorical levels in mind poses special problems. To help with this, MLJ provides a utility CategoricalDecoder which can decode a CategoricalArray into a plain array, and re-encode a prediction with the original levels intact. The CategoricalDecoder object created during fit will need to be bundled with fitresult to make it available to predict during re-encoding.","category":"page"},{"location":"adding_new_models/#","page":"Implementing the MLJ interface for a learning algorithm","title":"Implementing the MLJ interface for a learning algorithm","text":"So, for example, if the core algorithm being wrapped by fit expects a nominal target yint of type Vector{Int64} then the fit method may look something like this:","category":"page"},{"location":"adding_new_models/#","page":"Implementing the MLJ interface for a learning algorithm","title":"Implementing the MLJ interface for a learning algorithm","text":"function MLJBase.fit(model::SomeSupervisedModelType, verbosity, X, y)\n    decoder = MLJBase.CategoricalDecoder(y, Int64)\n    yint = transform(decoder, y)\n    core_fitresult = SomePackage.fit(X, yint, verbosity=verbosity)\n    fitresult = (decoder, core_fitresult)\n    cache = nothing\n    report = nothing\n    return fitresult, cache, report\nend","category":"page"},{"location":"adding_new_models/#","page":"Implementing the MLJ interface for a learning algorithm","title":"Implementing the MLJ interface for a learning algorithm","text":"while the corresponding predict operation might look like this:","category":"page"},{"location":"adding_new_models/#","page":"Implementing the MLJ interface for a learning algorithm","title":"Implementing the MLJ interface for a learning algorithm","text":"function MLJBase.predict(model::SomeSupervisedModelType, fitresult, Xnew)\n    decoder, core_fitresult = fitresult\n    yhat = SomePackage.predict(core_fitresult, Xnew)\n    return inverse_transform(decoder, yhat)\nend","category":"page"},{"location":"adding_new_models/#","page":"Implementing the MLJ interface for a learning algorithm","title":"Implementing the MLJ interface for a learning algorithm","text":"Query ?MLJBase.DecodeCategorical for more information.","category":"page"},{"location":"adding_new_models/#","page":"Implementing the MLJ interface for a learning algorithm","title":"Implementing the MLJ interface for a learning algorithm","text":"Prediction types for probabilistic responses. In the case of Probabilistic models, yhat must be a Vector whose elements are distributions (one distribution per row of Xnew).","category":"page"},{"location":"adding_new_models/#","page":"Implementing the MLJ interface for a learning algorithm","title":"Implementing the MLJ interface for a learning algorithm","text":"A distribution is any instance of a subtype of Distributions.Distribution from the package Distributions.jl, or any instance of the additional types UnivariateNominal and MultivariateNominal defined in MLJBase.jl (or any other type D for which MLJBase.isdistribution(::D) = true, meaning Base.rand and Distributions.pdf are implemented, as well Distributions.mean/Distribution.median or Distributions.mode).","category":"page"},{"location":"adding_new_models/#","page":"Implementing the MLJ interface for a learning algorithm","title":"Implementing the MLJ interface for a learning algorithm","text":"Use UnivariateNominal for Probabilistic classifiers with a single nominal target, whether binary or multiclass. For example, suppose levels(y)=[\"yes\", \"no\", \"maybe\"] and set L=levels(y). Then, if the predicted probabilities for some input pattern are [0.1, 0.7, 0.2], respectively, then the prediction returned for that pattern will be UnivariateNominal(L, [0.1, 0.7, 0.2]). Query ?UnivariateNominal for more information. ","category":"page"},{"location":"adding_new_models/#","page":"Implementing the MLJ interface for a learning algorithm","title":"Implementing the MLJ interface for a learning algorithm","text":"The predict method will need access to all levels of the target variable presented y presented for training, which consequently need to be encoded in the fitresult returned by fit. If a CategoricalDecoder object, decoder, has been bundled in fitresult, as in the deterministic example above, then the levels are given by levels(decoder).","category":"page"},{"location":"adding_new_models/#","page":"Implementing the MLJ interface for a learning algorithm","title":"Implementing the MLJ interface for a learning algorithm","text":"\n\n#### Trait declarations\n\nThere are a number of recommended trait declarations for each concrete\nsubtype `SomeSupervisedModel <: Supervised`. Basic fitting, resampling\nand tuning in MLJ does not require these traits but some advanced MLJ\nmeta-algorithms may require them now, or in the future. In particular,\nMLJ's `models(::Task)` method (matching models to user-specified\ntasks) can only identify models having a complete set of trait\ndeclarations. A full set of declarations are shown below for the\n`RidgeRegressor` type:\n","category":"page"},{"location":"adding_new_models/#","page":"Implementing the MLJ interface for a learning algorithm","title":"Implementing the MLJ interface for a learning algorithm","text":"julia MLJBase.outputkind(::Type{<:RidgeRegressor}) = :continuous MLJBase.outputquantity(::Type{<:RidgeRegressor}) = :univariate MLJBase.inputkinds(::Type{<:RidgeRegressor}) = [:continuous, ] MLJBase.inputquantity(::Type{<:RidgeRegressor}) = :multivariate MLJBase.ispurejulia(::Type{<:RidgeRegressor}) = :yes MLJBase.loadpath(::Type{<:RidgeRegressor}) = \"MLJ.RidgeRegressor\" MLJBase.packagename(::Type{<:RidgeRegressor}) = \"MultivariateStats\" MLJBase.packageuuid(::Type{<:RidgeRegressor}) = \"6f286f6a-111f-5878-ab1e-185364afe411\" MLJBase.packageurl((::Type{<:RidgeRegressor}) = \"https://github.com/JuliaStats/MultivariateStats.jl\"","category":"page"},{"location":"adding_new_models/#","page":"Implementing the MLJ interface for a learning algorithm","title":"Implementing the MLJ interface for a learning algorithm","text":"\nmethod                   | return type       | declarable return values | default value\n-------------------------|-------------------|---------------------------|-------------------\n`output_kind`            | `Symbol`          |`:continuous`, `:binary`, `:multiclass`, `:ordered_factor_finite`, `:ordered_factor_infinite`, `:mixed` | `:unknown`\n`output_quantity`        | `Symbol`          |`:univariate`, `:multivariate`| `:univariate`\n`input_kinds`          | `Vector{Symbol}`  | one or more of: `:continuous`, `:multiclass`, `:ordered_factor_finite`, `:ordered_factor_infinite`, `:missing` | `Symbol[]`\n`input_quantity`        | `Symbol`          | `:univariate`, `:multivariate` | `:multivariate`\n`is_pure_julia`          | `Symbol`          | `:yes`, `:no`             | `:unknown`\n`load_path`              | `String`          | unrestricted              | \"unknown\"\n`package_name`           | `String`          | unrestricted              | \"unknown\"\n`package_uuid`           | `String`          | unrestricted              | \"unknown\"\n`package_url`            | `String`          | unrestricted              | \"unknown\"\n\nNote that `:binary` does not mean *boolean*. Rather, it\nmeans the model is a classifier but is unable to classify targets with more than two\nclasses. As explained above, all classifiers are passed training targets\nas `CategoricalVector`s, whose element types are\narbitrary.\n\nThe option `:mixed` for `output_kind` is intended primarily\nfor transformers, such as MLJ's built-in `FeatureSelector`.\n\nYou can test declarations of traits by calling `info(SomeModelType)`.\n\nA `clean!` method may optionally be overloaded (the default returns an\nempty message without changing model fields):\nThis method is for checking and correcting invalid fields\n(hyperparameters) of the model, returning a warning `message`\nexplaining what has been changed. It should only throw an exception as\na last resort. This method should be called by the model keyword\nconstructor, as shown in the example below, and is called by MLJ\nbefore each call to `fit`.\n\n\n#### The update! method\n\nAn `update` method may be overloaded to enable a call by MLJ to\nretrain a model (on the same training data) to avoid repeating\ncomputations unnecessarily.\n","category":"page"},{"location":"adding_new_models/#","page":"Implementing the MLJ interface for a learning algorithm","title":"Implementing the MLJ interface for a learning algorithm","text":"julia MLJBase.update(model::SomeSupervisedModelType, verbosity, oldfitresult, oldcache, X, y) -> fitresult, cache, report ````","category":"page"},{"location":"adding_new_models/#","page":"Implementing the MLJ interface for a learning algorithm","title":"Implementing the MLJ interface for a learning algorithm","text":"If an MLJ Machine is being fit! and it is not the first time, then update is called instead of fit unless fit! has been called with new rows. However, MLJBase defines a fallback for update which just calls fit. For context, see \"MLJ Internals\". Learning networks wrapped as models constitute one use-case: One would like each component model to be retrained only when hyperparameter changes \"upstream\" make this necessary. In this case MLJ provides a fallback (specifically, the fallback is for any subtype of Supervised{Node}). A second important use-case is iterative models, where calls to increase the number of iterations only restarts the iterative procedure if other hyperparameters have also changed. For an example see builtins/Ensembles.jl.","category":"page"},{"location":"adding_new_models/#","page":"Implementing the MLJ interface for a learning algorithm","title":"Implementing the MLJ interface for a learning algorithm","text":"In the event that the argument fitresult (returned by a preceding call to fit) is not sufficient for performing an update, the author can arrange for fit to output in its cache return value any additional information required, as this is also passed as an argument to the update method.","category":"page"},{"location":"adding_new_models/#Multivariate-models-1","page":"Implementing the MLJ interface for a learning algorithm","title":"Multivariate models","text":"","category":"section"},{"location":"adding_new_models/#","page":"Implementing the MLJ interface for a learning algorithm","title":"Implementing the MLJ interface for a learning algorithm","text":"TODO","category":"page"},{"location":"adding_new_models/#","page":"Implementing the MLJ interface for a learning algorithm","title":"Implementing the MLJ interface for a learning algorithm","text":"<!– ##  Checklist for new adding models  –>","category":"page"},{"location":"adding_new_models/#","page":"Implementing the MLJ interface for a learning algorithm","title":"Implementing the MLJ interface for a learning algorithm","text":"<!– At present the following checklist is just for supervised models in –> <!– lazily loaded interface implementations. –>","category":"page"},{"location":"adding_new_models/#","page":"Implementing the MLJ interface for a learning algorithm","title":"Implementing the MLJ interface for a learning algorithm","text":"<!– - Copy and edit file –> <!– \"src/interfaces/DecisionTree.jl\" –> <!– which is annotated for use as a template. Give your new file a name –> <!– identical to the package name, including \".jl\" extension, such as –> <!– \"DecisionTree.jl\". Put this file in \"src/interfaces/\". –>","category":"page"},{"location":"adding_new_models/#","page":"Implementing the MLJ interface for a learning algorithm","title":"Implementing the MLJ interface for a learning algorithm","text":"<!– - Register your package for lazy loading with MLJ by finding out the –> <!– UUID of the package and adding an appropriate line to the __init__ –> <!– method at the end of \"src/MLJ.jl\". It will look something like this: –>","category":"page"},{"location":"adding_new_models/#","page":"Implementing the MLJ interface for a learning algorithm","title":"Implementing the MLJ interface for a learning algorithm","text":"<!– julia -- -- function __init__() -- --    load_interface DecisionTree 7806a523-6efd-50cb-b5f6-3fa6f1930dbb lazy=true -- --    load_interface NewExternalPackage 893749-98374-9234-91324-1324-9134-98 lazy=true -- -- end -- -- –>","category":"page"},{"location":"adding_new_models/#","page":"Implementing the MLJ interface for a learning algorithm","title":"Implementing the MLJ interface for a learning algorithm","text":"<!– With lazy=true, your glue code only gets loaded by the MLJ user –> <!– after they run import NewExternalPackage. For testing in your local –> <!– MLJ fork, you may want to set lazy=false but to use Revise you –> <!– will also need to move the @load_interface line out outside of the –> <!– __init__ function.  –>","category":"page"},{"location":"adding_new_models/#","page":"Implementing the MLJ interface for a learning algorithm","title":"Implementing the MLJ interface for a learning algorithm","text":"<!– - Write self-contained test-code for the methods defined in your glue –> <!– code, in a file with name like \"TestExternalPackage.jl\", but –> <!– placed in \"test/\". This code should be wrapped in a module to prevent –> <!– namespace conflicts with other test code. For a module name, just –> <!– prepend \"Test\", as in \"TestDecisionTree\". See \"test/TestDecisionTree.jl\" –> <!– for an example. –>","category":"page"},{"location":"adding_new_models/#","page":"Implementing the MLJ interface for a learning algorithm","title":"Implementing the MLJ interface for a learning algorithm","text":"<!– - Do not add the external package to the Project.toml file in the –> <!–   usual way. Rather, add its UUID to the [extras] section of –> <!–   Project.toml and add the package name to test = [Test\", \"DecisionTree\", --> <!--   ...]. –>","category":"page"},{"location":"adding_new_models/#","page":"Implementing the MLJ interface for a learning algorithm","title":"Implementing the MLJ interface for a learning algorithm","text":"<!– - Add suitable lines to \"test/runtests.jl\" to –> <!– include your test file, for the purpose of testing MLJ core and all –> <!– currently supported packages, including yours. You can Test your code –> <!– by running test MLJ from the Julia interactive package manager. You –> <!– will need to Pkg.dev your local MLJ fork first. To test your code in –> <!– isolation, locally edit \"test/runtest.jl\" appropriately. –>","category":"page"},{"location":"adding_new_models/#Unsupervised-models-1","page":"Implementing the MLJ interface for a learning algorithm","title":"Unsupervised models","text":"","category":"section"},{"location":"adding_new_models/#","page":"Implementing the MLJ interface for a learning algorithm","title":"Implementing the MLJ interface for a learning algorithm","text":"<!– TODO:","category":"page"},{"location":"adding_new_models/#","page":"Implementing the MLJ interface for a learning algorithm","title":"Implementing the MLJ interface for a learning algorithm","text":"specify that the output of predict/transform should be a table","category":"page"},{"location":"adding_new_models/#","page":"Implementing the MLJ interface for a learning algorithm","title":"Implementing the MLJ interface for a learning algorithm","text":"–>","category":"page"},{"location":"#MLJ.jl-1","page":"MLJ.jl","title":"MLJ.jl","text":"","category":"section"},{"location":"#","page":"MLJ.jl","title":"MLJ.jl","text":"Documentation for MLJ.jl","category":"page"},{"location":"#","page":"MLJ.jl","title":"MLJ.jl","text":"","category":"page"},{"location":"#Functions-1","page":"MLJ.jl","title":"Functions","text":"","category":"section"},{"location":"#","page":"MLJ.jl","title":"MLJ.jl","text":"@curve(var1, range, code)","category":"page"},{"location":"#MLJ.@curve-Tuple{Any,Any,Any}","page":"MLJ.jl","title":"MLJ.@curve","text":"@curve\n\nThe code,\n\n@curve var range code\n\nevaluates code, replacing appearances of var therein with each value in range. The range and corresponding evaluations are returned as a tuple of arrays. For example,\n\n@curve  x 1:3 (x^2 + 1)\n\nevaluates to\n\n([1,2,3], [2, 5, 10])\n\nThis is convenient for plotting functions using, eg, the Plots package:\n\nplot(@curve x 1:3 (x^2 + 1))\n\nA macro @pcurve parallelizes the same behaviour.  A two-variable implementation is also available, operating as in the following example:\n\njulia> @curve x [1,2,3] y [7,8] (x + y)\n([1,2,3],[7 8],[8.0 9.0; 9.0 10.0; 10.0 11.0])\n\njulia> ans[3]\n3×2 Array{Float64,2}:\n  8.0   9.0\n  9.0  10.0\n 10.0  11.0\n\nN.B. The second range is returned as a row vector for consistency with the output matrix. This is also helpful when plotting, as in:\n\njulia> u1, u2, A = @curve x range(0, stop=1, length=100) α [1,2,3] x^α\njulia> u2 = map(u2) do α \"α = \"*string(α) end\njulia> plot(u1, A, label=u2)\n\nwhich generates three superimposed plots - of the functions x, x^2 and x^3 - each labels with the exponents α = 1, 2, 3 in the legend.\n\n\n\n\n\n","category":"macro"},{"location":"#","page":"MLJ.jl","title":"MLJ.jl","text":"Modules = [MLJ]","category":"page"},{"location":"#MLJ.EnsembleModel-Tuple{}","page":"MLJ.jl","title":"MLJ.EnsembleModel","text":"EnsembleModel(atom=nothing, weights=Float64[], bagging_fraction=0.8, rng_seed=0, n=100, parallel=true)\n\nCreate a model for training an ensemble of n learners, with optional bagging, each with associated model atom. Ensembling is useful if fit!(machine(atom, data...)) does not create identical models on repeated calls (ie, is a stochastic model, such as a decision tree with randomized node selection criteria), or if bagging_fraction is set to a value not equal to 1.0 (or both). The constructor fails if no atom is specified.\n\nPredictions are weighted according to the vector weights (to allow for external optimization) except in the case that atom is a Deterministic classifier. Uniform weights are used if weight has zero length.\n\nThe ensemble model is Deterministic or Probabilistic, according to the corresponding supertype of atom. In the case of classifiers (targetscitype(atom) <: Union{Multiclass,FiniteOrderedFactor}), the predictions are majority votes, and for regressors (targetscitype(atom)<: Continuous) they are ordinary averages. Probabilistic predictions are obtained by averaging the atomic probability distribution functions; in particular, for regressors, the ensemble prediction on each input pattern has the type MixtureModel{VF,VS,D} from the Distributions.jl package, where D is the type of predicted distribution for atom.\n\n\n\n\n\n","category":"method"},{"location":"#MLJ.evaluate!-Tuple{Machine}","page":"MLJ.jl","title":"MLJ.evaluate!","text":"evaluate!(mach, resampling=CV(), measure=nothing, operation=predict, verbosity=1)\n\nEstimate the performance of a machine mach using the specified resampling (defaulting to 6-fold cross-validation) and measure. In general this mutating operation preserves only mach.args (the data stored in the machine).\n\nIf no measure is specified, then default_measure(mach.model) is used, unless this default is nothing and an error is thrown.\n\n\n\n\n\n","category":"method"},{"location":"#MLJ.iterator-Union{Tuple{M}, Tuple{M,Params}} where M<:MLJBase.Model","page":"MLJ.jl","title":"MLJ.iterator","text":"iterator(model::Model, param_iterators::Params)\n\nIterator over all models of type typeof(model) defined by param_iterators.\n\nEach name in the nested :name => value pairs of param_iterators should be the name of a (possibly nested) field of model; and each element of flat_values(param_iterators) (the corresponding final values) is an iterator over values of one of those fields.\n\nSee also iterator and params.\n\n\n\n\n\n","category":"method"},{"location":"#MLJ.learning_curve-Tuple{Supervised,Any,Vararg{Any,N} where N}","page":"MLJ.jl","title":"MLJ.learning_curve","text":"learning_curve(model, X, ys...; resolution=30, resampling=Holdout(), measure=rms, operation=pr, param_range=nothing)\n\nReturns (u, v) where u is a vector of hyperparameter values, and v the corresponding performance estimates. \n\nX, y = datanow()\natom = RidgeRegressor()\nmodel = EnsembleModel(atom=atom)\nr = range(atom, :lambda, lower=0.1, upper=100, scale=:log10)\nparam_range = Params(:atom => Params(:lambda => r))\nu, v = MLJ.learning_curve(model, X, y; param_range = param_range) \n\n\n\n\n\n","category":"method"},{"location":"#MLJ.models-Tuple{}","page":"MLJ.jl","title":"MLJ.models","text":"models()\n\nList the names of all MLJ models, loaded or registered, as a dictionary indexed on package name.\n\n\n\n\n\n","category":"method"},{"location":"#MLJ.rmsp-Tuple{AbstractArray{#s12,1} where #s12<:Real,Any}","page":"MLJ.jl","title":"MLJ.rmsp","text":"Root mean squared percentage loss \n\n\n\n\n\n","category":"method"},{"location":"#MLJ.strange-Tuple{MLJBase.Model,Symbol}","page":"MLJ.jl","title":"MLJ.strange","text":"strange(model, :hyper; kwargs...)\n\nReturns the pair :hyper => range(model, :hyper; kwargs...); \"strange\" is short for \"set to range\".\n\nSee also: range\n\n\n\n\n\n","category":"method"},{"location":"#MLJBase.info","page":"MLJ.jl","title":"MLJBase.info","text":"info(model, pkg=nothing)\n\nReturn the dictionary of metadata associated with model::String. If more than one package implements model then pkg::String will need to be specified.\n\n\n\n\n\n","category":"function"},{"location":"#MLJ.SCALE","page":"MLJ.jl","title":"MLJ.SCALE","text":"Scale = SCALE()\n\nObject for dispatching on scales and functions when generating parameter ranges. We require different behaviour for scales and functions:\n\n transform(Scale, scale(:log), 100) = 2\n inverse_transform(Scale, scale(:log), 2) = 100\n\nbut     transform(Scale, scale(log), 100) = 100       # identity     inverse_transform(Scale, scale(log), 100) = 2 \n\nSee also: strange\n\n\n\n\n\n","category":"type"},{"location":"#MLJ.SimpleDeterministicCompositeModel","page":"MLJ.jl","title":"MLJ.SimpleDeterministicCompositeModel","text":"SimpleDeterministicCompositeModel(;regressor=ConstantRegressor(), \n                          transformer=FeatureSelector())\n\nConstruct a composite model consisting of a transformer (Unsupervised model) followed by a Deterministic model. Mainly intended for internal testing .\n\n\n\n\n\n","category":"type"},{"location":"#Base.copy","page":"MLJ.jl","title":"Base.copy","text":"copy(params::Params, values=nothing)\n\nReturn a copy of params with new values. That is, flat_values(copy(params, values)) == values is true, while the first element of each nested pair (parameter name) is unchanged.\n\nIf values is not specified a deep copy is returned. \n\n\n\n\n\n","category":"function"},{"location":"#Base.merge!-Tuple{Array{T,1} where T,Array{T,1} where T}","page":"MLJ.jl","title":"Base.merge!","text":"merge!(tape1, tape2)\n\nIncrementally appends to tape1 all elements in tape2, excluding any element previously added (or any element of tape1 in its initial state).\n\n\n\n\n\n","category":"method"},{"location":"#Base.range-Union{Tuple{D}, Tuple{MLJType,Symbol}} where D","page":"MLJ.jl","title":"Base.range","text":"r = range(model, :hyper; values=nothing)\n\nDefines a NominalRange object for a field hyper of model. Note that r is not directly iterable but iterator(r) iterates over values.\n\nr = range(model, :hyper; upper=nothing, lower=nothing, scale=:linear)\n\nDefines a NumericRange object for a field hyper of model.  Note that r is not directly iteratable but iterator(r, n) iterates over n values between lower and upper values, according to the specified scale. The supported scales are :linear, :log, :log10, :log2. Values for Integer types are rounded (with duplicate values removed, resulting in possibly less than n values).\n\nAlternatively, if a function f is provided as scale, then iterator(r, n) iterates over the values [f(x1), f(x2), ... , f(xn)], where x1, x2, ..., xn are linearly spaced between lower and upper.\n\nSee also: strange, iterator\n\n\n\n\n\n","category":"method"},{"location":"#MLJ.flat_keys-Tuple{Pair{Symbol,B} where B}","page":"MLJ.jl","title":"MLJ.flat_keys","text":" flat_keys(params::Params)\n\nUse dot-concatentation to express each key in key-value pair of params in string form.\n\nExample\n\njulia> flat_keys(Params(:A => Params(:x => 2, :y => 3), :B)))\n[\"A.x\", \"A.y\", \"B\"]\n\n\n\n\n\n","category":"method"},{"location":"#MLJ.get_type-Tuple{Any,Symbol}","page":"MLJ.jl","title":"MLJ.get_type","text":"get_type(T, field::Symbol)\n\nReturns the type of the field field of DataType T. Not a type-stable function.  \n\n\n\n\n\n","category":"method"},{"location":"#MLJ.unwind-Tuple","page":"MLJ.jl","title":"MLJ.unwind","text":"unwind(iterators...)\n\nRepresent all possible combinations of values generated by iterators as rows of a matrix A. In more detail, A has one column for each iterator in iterators and one row for each distinct possible combination of values taken on by the iterators. Elements in the first column cycle fastest, those in the last clolumn slowest. \n\nExample\n\njulia> iterators = ([1, 2], [\"a\",\"b\"], [\"x\", \"y\", \"z\"]);\njulia> MLJ.unwind(iterators...)\n12×3 Array{Any,2}:\n 1  \"a\"  \"x\"\n 2  \"a\"  \"x\"\n 1  \"b\"  \"x\"\n 2  \"b\"  \"x\"\n 1  \"a\"  \"y\"\n 2  \"a\"  \"y\"\n 1  \"b\"  \"y\"\n 2  \"b\"  \"y\"\n 1  \"a\"  \"z\"\n 2  \"a\"  \"z\"\n 1  \"b\"  \"z\"\n 2  \"b\"  \"z\"\n\n\n\n\n\n","category":"method"},{"location":"#","page":"MLJ.jl","title":"MLJ.jl","text":"Modules = [MLJ.KNN]","category":"page"},{"location":"#Index-1","page":"MLJ.jl","title":"Index","text":"","category":"section"},{"location":"#","page":"MLJ.jl","title":"MLJ.jl","text":"","category":"page"},{"location":"internals/#MLJ-Internals-1","page":"MLJ Internals","title":"MLJ Internals","text":"","category":"section"},{"location":"internals/#The-machine-interface,-simplified-1","page":"MLJ Internals","title":"The machine interface, simplified","text":"","category":"section"},{"location":"internals/#","page":"MLJ Internals","title":"MLJ Internals","text":"The following is simplified description of the Machine interface. See also the glossary","category":"page"},{"location":"internals/#The-Machine-type-1","page":"MLJ Internals","title":"The Machine type","text":"","category":"section"},{"location":"internals/#","page":"MLJ Internals","title":"MLJ Internals","text":"mutable struct Machine{M<Model}\n\n\tmodel::M\n    fitresult\n    cache\n    args::Tuple    # e.g., (X, y) for supervised models\n    report\n    rows # remember last rows used \n    \n    function Machine{M}(model::M, args...) where M<:Model\n        machine = new{M}(model)\n        machine.args = args\n        machine.report = Dict{Symbol,Any}()\n        return machine\n    end\n\nend","category":"page"},{"location":"internals/#Constructor-1","page":"MLJ Internals","title":"Constructor","text":"","category":"section"},{"location":"internals/#","page":"MLJ Internals","title":"MLJ Internals","text":"machine(model::M, Xtable, y) = Machine{M}(model, Xtable, y)","category":"page"},{"location":"internals/#fit!-and-predict/transform-1","page":"MLJ Internals","title":"fit! and predict/transform","text":"","category":"section"},{"location":"internals/#","page":"MLJ Internals","title":"MLJ Internals","text":"function fit!(machine::Machine; rows=nothing, verbosity=1) \n\n    warning = clean!(mach.model)\n    isempty(warning) || verbosity < 0 || @warn warning \n\n    if rows == nothing\n        rows = (:) \n    end\n\n    rows_have_changed  = (!isdefined(mach, :rows) || rows != mach.rows)\n\n    args = [MLJ.selectrows(arg, rows) for arg in mach.args]\n\t\n    if !isdefined(mach, :fitresult) || rows_have_changed || force \n        mach.fitresult, mach.cache, report =\n            fit(mach.model, verbosity, args...)\n    else # call `update`:\n        mach.fitresult, mach.cache, report =\n            update(mach.model, verbosity, mach.fitresult, mach.cache, args...)\n    end\n\n    if rows_have_changed\n        mach.rows = deepcopy(rows)\n    end\n\n    if report != nothing\n        merge!(mach.report, report)\n    end\n\n    return mach\n\nend\n\nfunction predict(machine::Machine{<:Supervised}, Xnew)\n    if isdefined(machine, :fitresult)\n        return predict(machine.model, machine.fitresult, Xnew))\n    else\n        throw(error(\"$machine is not trained and so cannot predict.\"))\n    end\nend\n\nfunction transform(machine::Machine{<:Unsupervised}, Xnew)\n    if isdefined(machine, :fitresult)\n        return transform(machine.model, machine.fitresult, Xnew))\n    else\n        throw(error(\"$machine is not trained and so cannot transform.\"))\n    end\nend","category":"page"},{"location":"tiny_demo/#Basic-training-and-testing-1","page":"-","title":"Basic training and testing","text":"","category":"section"},{"location":"tiny_demo/#","page":"-","title":"-","text":"using MLJ\nusing DataFrames\n\ntask = load_boston()\nX, y = X_and_y(task);\n\nX = DataFrame(X) # or any other tabular format supported by Table.jl \n\ntrain, test = partition(eachindex(y), 0.7); # 70:30 split","category":"page"},{"location":"tiny_demo/#","page":"-","title":"-","text":"A model is a container for hyperparameters:","category":"page"},{"location":"tiny_demo/#","page":"-","title":"-","text":"knn_model=KNNRegressor(K=10)","category":"page"},{"location":"tiny_demo/#","page":"-","title":"-","text":"# KNNRegressor{Float64} @ 1…90: \ntarget_type             =>   Float64\nK                       =>   10\nmetric                  =>   euclidean (generic function with 1 method)\nkernel                  =>   reciprocal (generic function with 1 method)","category":"page"},{"location":"tiny_demo/#","page":"-","title":"-","text":"Wrapping the model in data creates a machine which will store training outcomes (called fit-results):","category":"page"},{"location":"tiny_demo/#","page":"-","title":"-","text":"knn = machine(knn_model, X, y)","category":"page"},{"location":"tiny_demo/#","page":"-","title":"-","text":"# Machine{KNNRegressor{Float64}} @ 9…72: \nmodel                   =>   KNNRegressor{Float64} @ 1…90\nfitresult               =>   (undefined)\ncache                   =>   (undefined)\nargs                    =>   (omitted Tuple{DataFrame,Array{Float64,1}} of length 2)\nreport                  =>   empty Dict{Symbol,Any}\nrows                    =>   (undefined)","category":"page"},{"location":"tiny_demo/#","page":"-","title":"-","text":"Training on the training rows and evaluating on the test rows:","category":"page"},{"location":"tiny_demo/#","page":"-","title":"-","text":"fit!(knn, rows=train)\nyhat = predict(knn, X[test,:])\nrms(y[test], yhat)","category":"page"},{"location":"tiny_demo/#","page":"-","title":"-","text":"┌ Info: Training Machine{KNNRegressor{Float64}} @ 9…72.\n└ @ MLJ /Users/anthony/Dropbox/Julia7/MLJ/src/machines.jl:69\n8.090639098853249","category":"page"},{"location":"tiny_demo/#","page":"-","title":"-","text":"Or, in one line:","category":"page"},{"location":"tiny_demo/#","page":"-","title":"-","text":"evaluate!(knn, resampling=Holdout(fraction_train=0.7))","category":"page"},{"location":"tiny_demo/#","page":"-","title":"-","text":"8.090639098853249","category":"page"},{"location":"tiny_demo/#","page":"-","title":"-","text":"Changing a hyperparameter and re-evaluating:","category":"page"},{"location":"tiny_demo/#","page":"-","title":"-","text":"knn_model.K = 20\nevaluate!(knn, resampling=Holdout(fraction_train=0.7))","category":"page"},{"location":"tiny_demo/#","page":"-","title":"-","text":"8.41003854724935","category":"page"},{"location":"tiny_demo/#Systematic-tuning-as-a-model-wrapper-1","page":"-","title":"Systematic tuning as a model wrapper","text":"","category":"section"},{"location":"tiny_demo/#","page":"-","title":"-","text":"A simple example of a composite model is a homogeneous ensemble. Here's a bagged ensemble model for 20 K-nearest neighbour regressors:","category":"page"},{"location":"tiny_demo/#","page":"-","title":"-","text":"ensemble_model = EnsembleModel(atom=knn_model, n=20) ","category":"page"},{"location":"tiny_demo/#","page":"-","title":"-","text":"# DeterministicEnsembleModel @ 5…24: \natom                    =>   KNNRegressor{Float64} @ 1…90\nweights                 =>   0-element Array{Float64,1}\nbagging_fraction        =>   0.8\nrng_seed                =>   0\nn                       =>   20\nparallel                =>   true","category":"page"},{"location":"tiny_demo/#","page":"-","title":"-","text":"Let's simultaneously tune the ensemble's bagging_fraction and the K-nearest neighbour hyperparameter K. Since one of these models is a field of the other, we have nested hyperparameters:","category":"page"},{"location":"tiny_demo/#","page":"-","title":"-","text":"params(ensemble_model)","category":"page"},{"location":"tiny_demo/#","page":"-","title":"-","text":"Params(:atom => Params(:target_type => Float64, :K => 20, :metric => MLJ.KNN.euclidean, :kernel => MLJ.KNN.reciprocal), :weights => Float64[], :bagging_fraction => 0.8, :rng_seed => 0, :n => 20, :parallel => true)","category":"page"},{"location":"tiny_demo/#","page":"-","title":"-","text":"To define a tuning grid, we construct ranges for the two parameters and collate these ranges following the same pattern above (omitting parameters that don't change):","category":"page"},{"location":"tiny_demo/#","page":"-","title":"-","text":"B_range = range(ensemble_model, :bagging_fraction, lower= 0.5, upper=1.0, scale = :linear)\nK_range = range(knn_model, :K, lower=1, upper=100, scale=:log10)\nnested_ranges = Params(:atom => Params(:K => K_range), :bagging_fraction => B_range)","category":"page"},{"location":"tiny_demo/#","page":"-","title":"-","text":"Params(:atom => Params(:K => NumericRange @ 1…75), :bagging_fraction => NumericRange @ 1…56)","category":"page"},{"location":"tiny_demo/#","page":"-","title":"-","text":"Now we choose a tuning strategy, and a resampling strategy (for estimating performance), and wrap these strategies around our ensemble model to obtain a new model:","category":"page"},{"location":"tiny_demo/#","page":"-","title":"-","text":"tuning = Grid(resolution=12)\nresampling = CV(nfolds=6)\n\ntuned_ensemble_model = TunedModel(model=ensemble_model, \n    tuning=tuning, resampling=resampling, nested_ranges=nested_ranges)","category":"page"},{"location":"tiny_demo/#","page":"-","title":"-","text":"# DeterministicTunedModel @ 1…93: \nmodel                   =>   DeterministicEnsembleModel @ 5…24\ntuning                  =>   Grid @ 1…37\nresampling              =>   CV @ 6…31\nmeasure                 =>   nothing\noperation               =>   predict (generic function with 19 methods)\nnested_ranges           =>   Params(:atom => Params(:K => NumericRange @ 1…75), :bagging_fraction => NumericRange @ 1…56)\nreport_measurements     =>   true","category":"page"},{"location":"tiny_demo/#","page":"-","title":"-","text":"Fitting the corresponding machine tunes the underlying model (in this case an ensemble) and retrains on all supplied data:","category":"page"},{"location":"tiny_demo/#","page":"-","title":"-","text":"tuned_ensemble = machine(tuned_ensemble_model, X[train,:], y[train])\nfit!(tuned_ensemble);","category":"page"},{"location":"tiny_demo/#","page":"-","title":"-","text":"┌ Info: Training Machine{MLJ.DeterministicTunedMo…} @ 1…05.\n└ @ MLJ /Users/anthony/Dropbox/Julia7/MLJ/src/machines.jl:69\nSearching a 132-point grid for best model: 100%[=========================] Time: 0:01:20\n┌ Info: Training best model on all supplied data.\n└ @ MLJ /Users/anthony/Dropbox/Julia7/MLJ/src/tuning.jl:130","category":"page"},{"location":"tiny_demo/#","page":"-","title":"-","text":"tuned_ensemble.report","category":"page"},{"location":"tiny_demo/#","page":"-","title":"-","text":"Dict{Symbol,Any} with 4 entries:\n  :measurements     => [7.03102, 6.09291, 6.05707, 5.93617, 5.86848, 5.73299, 5…\n  :models           => DeterministicEnsembleModel{Tuple{Array{Float64,2},Array{…\n  :best_model       => DeterministicEnsembleModel @ 3…49\n  :best_measurement => 5.46102","category":"page"},{"location":"tiny_demo/#","page":"-","title":"-","text":"best_model = tuned_ensemble.report[:best_model]\n@show best_model.bagging_fraction\n@show best_model.atom.K","category":"page"},{"location":"tiny_demo/#","page":"-","title":"-","text":"best_model.bagging_fraction = 0.7272727272727273\n(best_model.atom).K = 100","category":"page"}]
}
