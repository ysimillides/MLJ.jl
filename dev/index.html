<!DOCTYPE html>
<html lang="en"><head><meta charset="UTF-8"/><meta name="viewport" content="width=device-width, initial-scale=1.0"/><title>MLJ.jl · MLJ</title><link href="https://cdnjs.cloudflare.com/ajax/libs/normalize/4.2.0/normalize.min.css" rel="stylesheet" type="text/css"/><link href="https://fonts.googleapis.com/css?family=Lato|Roboto+Mono" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/4.6.3/css/font-awesome.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.12.0/styles/default.min.css" rel="stylesheet" type="text/css"/><script>documenterBaseURL="."</script><script src="https://cdnjs.cloudflare.com/ajax/libs/require.js/2.2.0/require.min.js" data-main="assets/documenter.js"></script><script src="siteinfo.js"></script><script src="../versions.js"></script><link href="assets/documenter.css" rel="stylesheet" type="text/css"/></head><body><nav class="toc"><h1>MLJ</h1><select id="version-selector" onChange="window.location.href=this.value" style="visibility: hidden"></select><form class="search" id="search-form" action="search/"><input id="search-query" name="q" type="text" placeholder="Search docs"/></form><ul><li><a class="toctext" href="adding_new_models/">Implementing the MLJ interface for a learning algorithm</a></li><li class="current"><a class="toctext" href>MLJ.jl</a><ul class="internal"><li><a class="toctext" href="#Functions-1">Functions</a></li><li><a class="toctext" href="#Index-1">Index</a></li></ul></li><li><a class="toctext" href="internals/">MLJ Internals</a></li><li><a class="toctext" href="tiny_demo/">-</a></li></ul></nav><article id="docs"><header><nav><ul><li><a href>MLJ.jl</a></li></ul><a class="edit-page" href="https://github.com/ysimillides/MLJ.jl/blob/master/docs/src/index.md"><span class="fa"></span> Edit on GitHub</a></nav><hr/><div id="topbar"><span>MLJ.jl</span><a class="fa fa-bars" href="#"></a></div></header><h1><a class="nav-anchor" id="MLJ.jl-1" href="#MLJ.jl-1">MLJ.jl</a></h1><p>Documentation for MLJ.jl</p><ul><li><a href="adding_new_models/#Implementing-the-MLJ-interface-for-a-learning-algorithm-1">Implementing the MLJ interface for a learning algorithm</a></li><ul><li><a href="adding_new_models/#Overview-1">Overview</a></li><li><a href="adding_new_models/#The-Model-API-1">The Model API</a></li></ul><li><a href="#MLJ.jl-1">MLJ.jl</a></li><ul><li><a href="#Functions-1">Functions</a></li><li><a href="#Index-1">Index</a></li></ul><li><a href="internals/#MLJ-Internals-1">MLJ Internals</a></li><ul><li><a href="internals/#The-machine-interface,-simplified-1">The machine interface, simplified</a></li></ul></ul><h2><a class="nav-anchor" id="Functions-1" href="#Functions-1">Functions</a></h2><section class="docstring"><div class="docstring-header"><a class="docstring-binding" id="MLJ.@curve-Tuple{Any,Any,Any}" href="#MLJ.@curve-Tuple{Any,Any,Any}"><code>MLJ.@curve</code></a> — <span class="docstring-category">Macro</span>.</div><div><div><p><strong><code>@curve</code></strong></p><p>The code,</p><pre><code class="language-none">@curve var range code</code></pre><p>evaluates <code>code</code>, replacing appearances of <code>var</code> therein with each value in <code>range</code>. The range and corresponding evaluations are returned as a tuple of arrays. For example,</p><pre><code class="language-none">@curve  x 1:3 (x^2 + 1)</code></pre><p>evaluates to</p><pre><code class="language-none">([1,2,3], [2, 5, 10])</code></pre><p>This is convenient for plotting functions using, eg, the <code>Plots</code> package:</p><pre><code class="language-none">plot(@curve x 1:3 (x^2 + 1))</code></pre><p>A macro <code>@pcurve</code> parallelizes the same behaviour.  A two-variable implementation is also available, operating as in the following example:</p><pre><code class="language-none">julia&gt; @curve x [1,2,3] y [7,8] (x + y)
([1,2,3],[7 8],[8.0 9.0; 9.0 10.0; 10.0 11.0])

julia&gt; ans[3]
3×2 Array{Float64,2}:
  8.0   9.0
  9.0  10.0
 10.0  11.0</code></pre><p>N.B. The second range is returned as a <em>row</em> vector for consistency with the output matrix. This is also helpful when plotting, as in:</p><pre><code class="language-none">julia&gt; u1, u2, A = @curve x range(0, stop=1, length=100) α [1,2,3] x^α
julia&gt; u2 = map(u2) do α &quot;α = &quot;*string(α) end
julia&gt; plot(u1, A, label=u2)</code></pre><p>which generates three superimposed plots - of the functions x, x^2 and x^3 - each labels with the exponents α = 1, 2, 3 in the legend.</p></div></div><a class="source-link" target="_blank" href="https://github.com/alan-turing-institute/MLJ.jl/blob/97bca407b226888b696a3212ee32dc926a4304cd/src/utilities.jl#L49-L93">source</a></section><section class="docstring"><div class="docstring-header"><a class="docstring-binding" id="MLJ.EnsembleModel-Tuple{}" href="#MLJ.EnsembleModel-Tuple{}"><code>MLJ.EnsembleModel</code></a> — <span class="docstring-category">Method</span>.</div><div><div><pre><code class="language-none">EnsembleModel(atom=nothing, weights=Float64[], bagging_fraction=0.8, rng_seed=0, n=100, parallel=true)</code></pre><p>Create a model for training an ensemble of <code>n</code> learners, with optional bagging, each with associated model <code>atom</code>. Ensembling is useful if <code>fit!(machine(atom, data...))</code> does not create identical models on repeated calls (ie, is a stochastic model, such as a decision tree with randomized node selection criteria), or if <code>bagging_fraction</code> is set to a value not equal to 1.0 (or both). The constructor fails if no <code>atom</code> is specified.</p><p>Predictions are weighted according to the vector <code>weights</code> (to allow for external optimization) except in the case that <code>atom</code> is a <code>Deterministic</code> classifier. Uniform weights are used if <code>weight</code> has zero length.</p><p>The ensemble model is <code>Deterministic</code> or <code>Probabilistic</code>, according to the corresponding supertype of <code>atom</code>. In the case of classifiers (target<em>scitype(atom) &lt;: Union{Multiclass,FiniteOrderedFactor}), the predictions are majority votes, and for regressors (target</em>scitype(atom)&lt;: Continuous) they are ordinary averages. Probabilistic predictions are obtained by averaging the atomic probability distribution functions; in particular, for regressors, the ensemble prediction on each input pattern has the type <code>MixtureModel{VF,VS,D}</code> from the Distributions.jl package, where <code>D</code> is the type of predicted distribution for <code>atom</code>.</p></div></div><a class="source-link" target="_blank" href="https://github.com/alan-turing-institute/MLJ.jl/blob/97bca407b226888b696a3212ee32dc926a4304cd/src/ensembles.jl#L234-L261">source</a></section><section class="docstring"><div class="docstring-header"><a class="docstring-binding" id="MLJ.evaluate!-Tuple{Machine}" href="#MLJ.evaluate!-Tuple{Machine}"><code>MLJ.evaluate!</code></a> — <span class="docstring-category">Method</span>.</div><div><div><pre><code class="language-none">evaluate!(mach, resampling=CV(), measure=nothing, operation=predict, verbosity=1)</code></pre><p>Estimate the performance of a machine <code>mach</code> using the specified <code>resampling</code> (defaulting to 6-fold cross-validation) and <code>measure</code>. In general this mutating operation preserves only <code>mach.args</code> (the data stored in the machine).</p><p>If no measure is specified, then <code>default_measure(mach.model)</code> is used, unless this default is <code>nothing</code> and an error is thrown.</p></div></div><a class="source-link" target="_blank" href="https://github.com/alan-turing-institute/MLJ.jl/blob/97bca407b226888b696a3212ee32dc926a4304cd/src/resampling.jl#L42-L53">source</a></section><section class="docstring"><div class="docstring-header"><a class="docstring-binding" id="MLJ.iterator-Union{Tuple{M}, Tuple{M,Params}} where M&lt;:MLJBase.Model" href="#MLJ.iterator-Union{Tuple{M}, Tuple{M,Params}} where M&lt;:MLJBase.Model"><code>MLJ.iterator</code></a> — <span class="docstring-category">Method</span>.</div><div><div><pre><code class="language-none">iterator(model::Model, param_iterators::Params)</code></pre><p>Iterator over all models of type <code>typeof(model)</code> defined by <code>param_iterators</code>.</p><p>Each <code>name</code> in the nested <code>:name =&gt; value</code> pairs of <code>param_iterators</code> should be the name of a (possibly nested) field of <code>model</code>; and each element of <code>flat_values(param_iterators)</code> (the corresponding final values) is an iterator over values of one of those fields.</p><p>See also <code>iterator</code> and <code>params</code>.</p></div></div><a class="source-link" target="_blank" href="https://github.com/alan-turing-institute/MLJ.jl/blob/97bca407b226888b696a3212ee32dc926a4304cd/src/parameters.jl#L347-L360">source</a></section><section class="docstring"><div class="docstring-header"><a class="docstring-binding" id="MLJ.learning_curve-Tuple{Supervised,Any,Vararg{Any,N} where N}" href="#MLJ.learning_curve-Tuple{Supervised,Any,Vararg{Any,N} where N}"><code>MLJ.learning_curve</code></a> — <span class="docstring-category">Method</span>.</div><div><div><pre><code class="language-none">learning_curve(model, X, ys...; resolution=30, resampling=Holdout(), measure=rms, operation=pr, param_range=nothing)</code></pre><p>Returns <code>(u, v)</code> where <code>u</code> is a vector of hyperparameter values, and <code>v</code> the corresponding performance estimates. </p><pre><code class="language-julia">X, y = datanow()
atom = RidgeRegressor()
model = EnsembleModel(atom=atom)
r = range(atom, :lambda, lower=0.1, upper=100, scale=:log10)
param_range = Params(:atom =&gt; Params(:lambda =&gt; r))
u, v = MLJ.learning_curve(model, X, y; param_range = param_range) </code></pre></div></div><a class="source-link" target="_blank" href="https://github.com/alan-turing-institute/MLJ.jl/blob/97bca407b226888b696a3212ee32dc926a4304cd/src/tuning.jl#L198-L213">source</a></section><section class="docstring"><div class="docstring-header"><a class="docstring-binding" id="MLJ.models-Tuple{}" href="#MLJ.models-Tuple{}"><code>MLJ.models</code></a> — <span class="docstring-category">Method</span>.</div><div><div><pre><code class="language-none">models()</code></pre><p>List the names of all MLJ models, loaded or registered, as a dictionary indexed on package name.</p></div></div><a class="source-link" target="_blank" href="https://github.com/alan-turing-institute/MLJ.jl/blob/97bca407b226888b696a3212ee32dc926a4304cd/src/loading.jl#L37-L41">source</a></section><section class="docstring"><div class="docstring-header"><a class="docstring-binding" id="MLJ.rmsp-Tuple{AbstractArray{#s12,1} where #s12&lt;:Real,Any}" href="#MLJ.rmsp-Tuple{AbstractArray{#s12,1} where #s12&lt;:Real,Any}"><code>MLJ.rmsp</code></a> — <span class="docstring-category">Method</span>.</div><div><div><p>Root mean squared percentage loss </p></div></div><a class="source-link" target="_blank" href="https://github.com/alan-turing-institute/MLJ.jl/blob/97bca407b226888b696a3212ee32dc926a4304cd/src/metrics.jl#L58">source</a></section><section class="docstring"><div class="docstring-header"><a class="docstring-binding" id="MLJ.strange-Tuple{MLJBase.Model,Symbol}" href="#MLJ.strange-Tuple{MLJBase.Model,Symbol}"><code>MLJ.strange</code></a> — <span class="docstring-category">Method</span>.</div><div><div><pre><code class="language-none">strange(model, :hyper; kwargs...)</code></pre><p>Returns the pair <code>:hyper =&gt; range(model, :hyper; kwargs...)</code>; &quot;strange&quot; is short for &quot;set to range&quot;.</p><p>See also: range</p></div></div><a class="source-link" target="_blank" href="https://github.com/alan-turing-institute/MLJ.jl/blob/97bca407b226888b696a3212ee32dc926a4304cd/src/parameters.jl#L254-L262">source</a></section><section class="docstring"><div class="docstring-header"><a class="docstring-binding" id="MLJBase.info" href="#MLJBase.info"><code>MLJBase.info</code></a> — <span class="docstring-category">Function</span>.</div><div><div><p>info(model, pkg=nothing)</p><p>Return the dictionary of metadata associated with <code>model::String</code>. If more than one package implements <code>model</code> then <code>pkg::String</code> will need to be specified.</p></div></div><a class="source-link" target="_blank" href="https://github.com/alan-turing-institute/MLJ.jl/blob/97bca407b226888b696a3212ee32dc926a4304cd/src/loading.jl#L52-L59">source</a></section><section class="docstring"><div class="docstring-header"><a class="docstring-binding" id="MLJ.SCALE" href="#MLJ.SCALE"><code>MLJ.SCALE</code></a> — <span class="docstring-category">Type</span>.</div><div><div><pre><code class="language-none">Scale = SCALE()</code></pre><p>Object for dispatching on scales and functions when generating parameter ranges. We require different behaviour for scales and functions:</p><pre><code class="language-none"> transform(Scale, scale(:log), 100) = 2
 inverse_transform(Scale, scale(:log), 2) = 100</code></pre><p>but     transform(Scale, scale(log), 100) = 100       # identity     inverse_transform(Scale, scale(log), 100) = 2 </p><p>See also: strange</p></div></div><a class="source-link" target="_blank" href="https://github.com/alan-turing-institute/MLJ.jl/blob/97bca407b226888b696a3212ee32dc926a4304cd/src/parameters.jl#L157-L175">source</a></section><section class="docstring"><div class="docstring-header"><a class="docstring-binding" id="MLJ.SimpleDeterministicCompositeModel" href="#MLJ.SimpleDeterministicCompositeModel"><code>MLJ.SimpleDeterministicCompositeModel</code></a> — <span class="docstring-category">Type</span>.</div><div><div><pre><code class="language-none">SimpleDeterministicCompositeModel(;regressor=ConstantRegressor(), 
                          transformer=FeatureSelector())</code></pre><p>Construct a composite model consisting of a transformer (<code>Unsupervised</code> model) followed by a <code>Deterministic</code> model. Mainly intended for internal testing .</p></div></div><a class="source-link" target="_blank" href="https://github.com/alan-turing-institute/MLJ.jl/blob/97bca407b226888b696a3212ee32dc926a4304cd/src/composites.jl#L12-L20">source</a></section><section class="docstring"><div class="docstring-header"><a class="docstring-binding" id="Base.copy" href="#Base.copy"><code>Base.copy</code></a> — <span class="docstring-category">Function</span>.</div><div><div><pre><code class="language-none">copy(params::Params, values=nothing)</code></pre><p>Return a copy of <code>params</code> with new <code>values</code>. That is, <code>flat_values(copy(params, values)) == values</code> is true, while the first element of each nested pair (parameter name) is unchanged.</p><p>If <code>values</code> is not specified a deep copy is returned. </p></div></div><a class="source-link" target="_blank" href="https://github.com/alan-turing-institute/MLJ.jl/blob/97bca407b226888b696a3212ee32dc926a4304cd/src/parameters.jl#L117-L126">source</a></section><section class="docstring"><div class="docstring-header"><a class="docstring-binding" id="Base.merge!-Tuple{Array{T,1} where T,Array{T,1} where T}" href="#Base.merge!-Tuple{Array{T,1} where T,Array{T,1} where T}"><code>Base.merge!</code></a> — <span class="docstring-category">Method</span>.</div><div><div><pre><code class="language-none">merge!(tape1, tape2)</code></pre><p>Incrementally appends to <code>tape1</code> all elements in <code>tape2</code>, excluding any element previously added (or any element of <code>tape1</code> in its initial state).</p></div></div><a class="source-link" target="_blank" href="https://github.com/alan-turing-institute/MLJ.jl/blob/97bca407b226888b696a3212ee32dc926a4304cd/src/networks.jl#L28-L36">source</a></section><section class="docstring"><div class="docstring-header"><a class="docstring-binding" id="Base.range-Union{Tuple{D}, Tuple{MLJType,Symbol}} where D" href="#Base.range-Union{Tuple{D}, Tuple{MLJType,Symbol}} where D"><code>Base.range</code></a> — <span class="docstring-category">Method</span>.</div><div><div><pre><code class="language-none">r = range(model, :hyper; values=nothing)</code></pre><p>Defines a <code>NominalRange</code> object for a field <code>hyper</code> of <code>model</code>. Note that <code>r</code> is not directly iterable but <code>iterator(r)</code> iterates over <code>values</code>.</p><pre><code class="language-none">r = range(model, :hyper; upper=nothing, lower=nothing, scale=:linear)</code></pre><p>Defines a <code>NumericRange</code> object for a field <code>hyper</code> of <code>model</code>.  Note that <code>r</code> is not directly iteratable but <code>iterator(r, n)</code> iterates over <code>n</code> values between <code>lower</code> and <code>upper</code> values, according to the specified <code>scale</code>. The supported scales are <code>:linear, :log, :log10, :log2</code>. Values for <code>Integer</code> types are rounded (with duplicate values removed, resulting in possibly less than <code>n</code> values).</p><p>Alternatively, if a function <code>f</code> is provided as <code>scale</code>, then <code>iterator(r, n)</code> iterates over the values <code>[f(x1), f(x2), ... , f(xn)]</code>, where <code>x1, x2, ..., xn</code> are linearly spaced between <code>lower</code> and <code>upper</code>.</p><p>See also: strange, iterator</p></div></div><a class="source-link" target="_blank" href="https://github.com/alan-turing-institute/MLJ.jl/blob/97bca407b226888b696a3212ee32dc926a4304cd/src/parameters.jl#L217-L240">source</a></section><section class="docstring"><div class="docstring-header"><a class="docstring-binding" id="MLJ.flat_keys-Tuple{Pair{Symbol,B} where B}" href="#MLJ.flat_keys-Tuple{Pair{Symbol,B} where B}"><code>MLJ.flat_keys</code></a> — <span class="docstring-category">Method</span>.</div><div><div><pre><code class="language-none"> flat_keys(params::Params)</code></pre><p>Use dot-concatentation to express each key in key-value pair of <code>params</code> in string form.</p><p><strong>Example</strong></p><pre><code class="language-none">julia&gt; flat_keys(Params(:A =&gt; Params(:x =&gt; 2, :y =&gt; 3), :B)))
[&quot;A.x&quot;, &quot;A.y&quot;, &quot;B&quot;]</code></pre></div></div><a class="source-link" target="_blank" href="https://github.com/alan-turing-institute/MLJ.jl/blob/97bca407b226888b696a3212ee32dc926a4304cd/src/parameters.jl#L95-L108">source</a></section><section class="docstring"><div class="docstring-header"><a class="docstring-binding" id="MLJ.get_type-Tuple{Any,Symbol}" href="#MLJ.get_type-Tuple{Any,Symbol}"><code>MLJ.get_type</code></a> — <span class="docstring-category">Method</span>.</div><div><div><p>get_type(T, field::Symbol)</p><p>Returns the type of the field <code>field</code> of <code>DataType</code> T. Not a type-stable function.  </p></div></div><a class="source-link" target="_blank" href="https://github.com/alan-turing-institute/MLJ.jl/blob/97bca407b226888b696a3212ee32dc926a4304cd/src/parameters.jl#L202-L209">source</a></section><section class="docstring"><div class="docstring-header"><a class="docstring-binding" id="MLJ.unwind-Tuple" href="#MLJ.unwind-Tuple"><code>MLJ.unwind</code></a> — <span class="docstring-category">Method</span>.</div><div><div><pre><code class="language-none">unwind(iterators...)</code></pre><p>Represent all possible combinations of values generated by <code>iterators</code> as rows of a matrix <code>A</code>. In more detail, <code>A</code> has one column for each iterator in <code>iterators</code> and one row for each distinct possible combination of values taken on by the iterators. Elements in the first column cycle fastest, those in the last clolumn slowest. </p><p><strong>Example</strong></p><pre><code class="language-julia">julia&gt; iterators = ([1, 2], [&quot;a&quot;,&quot;b&quot;], [&quot;x&quot;, &quot;y&quot;, &quot;z&quot;]);
julia&gt; MLJ.unwind(iterators...)
12×3 Array{Any,2}:
 1  &quot;a&quot;  &quot;x&quot;
 2  &quot;a&quot;  &quot;x&quot;
 1  &quot;b&quot;  &quot;x&quot;
 2  &quot;b&quot;  &quot;x&quot;
 1  &quot;a&quot;  &quot;y&quot;
 2  &quot;a&quot;  &quot;y&quot;
 1  &quot;b&quot;  &quot;y&quot;
 2  &quot;b&quot;  &quot;y&quot;
 1  &quot;a&quot;  &quot;z&quot;
 2  &quot;a&quot;  &quot;z&quot;
 1  &quot;b&quot;  &quot;z&quot;
 2  &quot;b&quot;  &quot;z&quot;</code></pre></div></div><a class="source-link" target="_blank" href="https://github.com/alan-turing-institute/MLJ.jl/blob/97bca407b226888b696a3212ee32dc926a4304cd/src/parameters.jl#L296-L325">source</a></section><h2><a class="nav-anchor" id="Index-1" href="#Index-1">Index</a></h2><ul><li><a href="#MLJ.SCALE"><code>MLJ.SCALE</code></a></li><li><a href="#MLJ.SimpleDeterministicCompositeModel"><code>MLJ.SimpleDeterministicCompositeModel</code></a></li><li><a href="#Base.copy"><code>Base.copy</code></a></li><li><a href="#Base.merge!-Tuple{Array{T,1} where T,Array{T,1} where T}"><code>Base.merge!</code></a></li><li><a href="#Base.range-Union{Tuple{D}, Tuple{MLJType,Symbol}} where D"><code>Base.range</code></a></li><li><a href="#MLJ.EnsembleModel-Tuple{}"><code>MLJ.EnsembleModel</code></a></li><li><a href="#MLJ.evaluate!-Tuple{Machine}"><code>MLJ.evaluate!</code></a></li><li><a href="#MLJ.flat_keys-Tuple{Pair{Symbol,B} where B}"><code>MLJ.flat_keys</code></a></li><li><a href="#MLJ.get_type-Tuple{Any,Symbol}"><code>MLJ.get_type</code></a></li><li><a href="#MLJ.iterator-Union{Tuple{M}, Tuple{M,Params}} where M&lt;:MLJBase.Model"><code>MLJ.iterator</code></a></li><li><a href="#MLJ.learning_curve-Tuple{Supervised,Any,Vararg{Any,N} where N}"><code>MLJ.learning_curve</code></a></li><li><a href="#MLJ.models-Tuple{}"><code>MLJ.models</code></a></li><li><a href="#MLJ.rmsp-Tuple{AbstractArray{#s12,1} where #s12&lt;:Real,Any}"><code>MLJ.rmsp</code></a></li><li><a href="#MLJ.strange-Tuple{MLJBase.Model,Symbol}"><code>MLJ.strange</code></a></li><li><a href="#MLJ.unwind-Tuple"><code>MLJ.unwind</code></a></li><li><a href="#MLJBase.info"><code>MLJBase.info</code></a></li><li><a href="#MLJ.@curve-Tuple{Any,Any,Any}"><code>MLJ.@curve</code></a></li></ul><footer><hr/><a class="previous" href="adding_new_models/"><span class="direction">Previous</span><span class="title">Implementing the MLJ interface for a learning algorithm</span></a><a class="next" href="internals/"><span class="direction">Next</span><span class="title">MLJ Internals</span></a></footer></article></body></html>
