<!DOCTYPE html>
<html lang="en"><head><meta charset="UTF-8"/><meta name="viewport" content="width=device-width, initial-scale=1.0"/><title>Implementing the MLJ interface for a learning algorithm · MLJ</title><link href="https://cdnjs.cloudflare.com/ajax/libs/normalize/4.2.0/normalize.min.css" rel="stylesheet" type="text/css"/><link href="https://fonts.googleapis.com/css?family=Lato|Roboto+Mono" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/4.6.3/css/font-awesome.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.12.0/styles/default.min.css" rel="stylesheet" type="text/css"/><script>documenterBaseURL=".."</script><script src="https://cdnjs.cloudflare.com/ajax/libs/require.js/2.2.0/require.min.js" data-main="../assets/documenter.js"></script><script src="../siteinfo.js"></script><script src="../../versions.js"></script><link href="../assets/documenter.css" rel="stylesheet" type="text/css"/></head><body><nav class="toc"><h1>MLJ</h1><select id="version-selector" onChange="window.location.href=this.value" style="visibility: hidden"></select><form class="search" id="search-form" action="../search/"><input id="search-query" name="q" type="text" placeholder="Search docs"/></form><ul><li class="current"><a class="toctext" href>Implementing the MLJ interface for a learning algorithm</a><ul class="internal"><li><a class="toctext" href="#Overview-1">Overview</a></li><li><a class="toctext" href="#The-Model-API-1">The Model API</a></li></ul></li><li><a class="toctext" href="../">MLJ.jl</a></li><li><a class="toctext" href="../internals/">MLJ Internals</a></li><li><a class="toctext" href="../tiny_demo/">-</a></li></ul></nav><article id="docs"><header><nav><ul><li><a href>Implementing the MLJ interface for a learning algorithm</a></li></ul><a class="edit-page" href="https://github.com/ysimillides/MLJ.jl/blob/master/docs/src/adding_new_models.md"><span class="fa"></span> Edit on GitHub</a></nav><hr/><div id="topbar"><span>Implementing the MLJ interface for a learning algorithm</span><a class="fa fa-bars" href="#"></a></div></header><h1><a class="nav-anchor" id="Implementing-the-MLJ-interface-for-a-learning-algorithm-1" href="#Implementing-the-MLJ-interface-for-a-learning-algorithm-1">Implementing the MLJ interface for a learning algorithm</a></h1><p>This guide outlines the specification of the MLJ model interface. The machine learning tools provided by MLJ can be applied to the models in any package that imports the module <a href="https://github.com/alan-turing-institute/MLJBase.jl">MLJBase</a> and implements the API defined there as outlined below.</p><p>To implement the API described here, some familiarity with the following packages is helpful: <a href="https://github.com/JuliaStats/Distributions.jl">Distributions.jl</a> (for probabilistic predictions), <a href="https://github.com/JuliaData/CategoricalArrays.jl">CategoricalArrays.jl</a> (essential if you are implementing a classifier, or a learner that handles categorical inputs), <a href="https://github.com/JuliaData/Tables.jl">Tables.jl</a> (if you&#39;re algorithm needs input data in a novel format).</p><p>For a quick and dirty implementation of user-defined models see <a href=".">here</a>.</p><p>&lt;!– As a temporary measure, –&gt; &lt;!– the MLJ package also implements the MLJ interface for some –&gt; &lt;!– non-compliant packages, using lazily loaded modules (&quot;glue code&quot;) –&gt; &lt;!– residing in –&gt; &lt;!– <a href="https://github.com/alan-turing-institute/MLJ.jl/tree/master/src/interfaces">src/interfaces</a> –&gt; &lt;!– of the MLJ.jl repository. A checklist for adding models in this latter –&gt; &lt;!– way is given at the end; a template is given here: –&gt; &lt;!– <a href="https://github.com/alan-turing-institute/MLJ.jl/tree/master/src/interfaces/DecisionTree.jl">&quot;src/interfaces/DecisionTree.jl&quot;</a>. –&gt;</p><p>In MLJ, the basic interface exposed to the user, built atop the model interface described here, is the <em>machine interface</em>. After a first reading of this document, the reader may wish to refer to the simplified description of the machine interface appearing under <a href="../internals/">&quot;MLJ Internals&quot;</a>.</p><p>&lt;!– ### MLJ types –&gt;</p><p>&lt;!– Every type introduced the core MLJ package should be a subtype of: –&gt;</p><p>&lt;!– <code>--&gt; &lt;!-- abstract type MLJType end --&gt; &lt;!--</code> –&gt;</p><p>&lt;!– The Julia <code>show</code> method is informatively overloaded for this –&gt; &lt;!– type. Variable bindings declared with <code>@constant</code> &quot;register&quot; the –&gt; &lt;!– binding, which is reflected in the output of <code>show</code>. –&gt;</p><h2><a class="nav-anchor" id="Overview-1" href="#Overview-1">Overview</a></h2><p>A <em>model</em> is an object storing hyperparameters associated with some machine learning algorithm, where &quot;learning algorithm&quot; is broadly interpreted.  In MLJ, hyperparameters include configuration parameters, like the number of threads, and special instructions, such as &quot;compute feature rankings&quot;, which may or may not affect the final learning outcome.  However, the logging level (<code>verbosity</code> below) is excluded.</p><p>The name of the Julia type associated with a model indicates the associated algorithm (e.g., <code>DecisionTreeClassifier</code>). The outcome of training a learning algorithm is here called a <em>fit-result</em>. For ordinary multilinear regression, for example, this would be the coefficients and intercept. For a general supervised model, it is the (generally minimal) information needed to make new predictions.</p><p>The ultimate supertype of all models is <code>MLJBase.Model</code>, which has two abstract subtypes; quoting MLJBase.jl:</p><pre><code class="language-julia">abstract type MLJType end
abstract type Supervised{R} &lt;: Model end
abstract type Unsupervised &lt;: Model end</code></pre><p>Here the parameter <code>R</code> refers to a fit-result type. By declaring a model to be a subtype of <code>MLJBase.Supervised{R}</code> you guarantee the fit-result to be of type <code>R</code> and, if <code>R</code> is concrete, one may improve the performance of homogeneous ensembles of the model (as defined by the built-in MLJ <code>EnsembleModel</code> wrapper). There is no abstract type for fit-results because these types are generally declared outside of MLJBase.</p><p><code>Supervised</code> models are further divided according to whether they are able to furnish probabilistic predictions of the target(s) (which they will do so by default) or directly predict &quot;point&quot; estimates, for each new input pattern:</p><pre><code class="language-julia">abstract type Probabilistic{R} &lt;: Supervised{R} end
abstract type Deterministic{R} &lt;: Supervised{R} end</code></pre><p>Further division of model types is realized through &quot;trait&quot; declarations (see below). </p><p>Associated with every concrete subtype of <code>Model</code> there must be a <code>fit</code> method, which implements the associated algorithm to produce the fit-result. Additionally, every <code>Supervised</code> model has a <code>predict</code> method, while <code>Unsupervised</code> models must have a <code>transform</code> method. More generally, methods such as these, that are dispatched on a model instance and a fit-result (plus other data), are called <em>operations</em>. <code>Probabilistic</code> supervised models optionally implement a <code>predict_mode</code> operation (in the case of classifiers) or a <code>predict_mean</code> and/or <code>predict_median</code> operations (in the case of regressors) overriding obvious fallbacks provided by <code>MLJBase</code>. <code>Unsupervised</code> models may implement an <code>inverse_transform</code> operation.</p><h2><a class="nav-anchor" id="The-Model-API-1" href="#The-Model-API-1">The Model API</a></h2><p>&lt;!– Every package interface should live inside a submodule for namespace –&gt; &lt;!– hygiene (see the template at –&gt; &lt;!– &quot;src/interfaces/DecisionTree.jl&quot;). Ideally, package interfaces should –&gt; &lt;!– export no <code>struct</code> outside of the new model types they define, and –&gt; &lt;!– import only abstract types. All &quot;structural&quot; design should be –&gt; &lt;!– restricted to the MLJ core to prevent rewriting glue code when there –&gt; &lt;!– are design changes. –&gt;</p><h3><a class="nav-anchor" id="New-model-type-declarations-and-optional-clean!-method-1" href="#New-model-type-declarations-and-optional-clean!-method-1">New model type declarations and optional clean! method</a></h3><p>Here is an example of a concrete supervised model type declaration, made after defining an appropriate fit-result type (an optional step):</p><pre><code class="language-julia">import MLJ

struct LinearFitResult{F&lt;:AbstractFloat} &lt;: MLJBase.MLJType
    coefficients::Vector{F}
    bias::F
end

mutable struct RidgeRegressor{F} &lt;: MLJBase.Deterministic{LinearFitResult{F}}
    target_type::Type{F}
    lambda::Float64
end</code></pre><p>Models (which are mutable) should not be given internal constructors. It is recommended that they be given an external lazy keyword constructor of the same name. This constructor defines default values for every field, and optionally corrects invalid field values by calling a <code>clean!</code> method (whose fallback returns an empty message string):</p><pre><code class="language-julia">function MLJ.clean!(model::RidgeRegressor)
    warning = &quot;&quot;
    if model.lambda &lt; 0
        warning *= &quot;Need lambda ≥ 0. Resetting lambda=0. &quot;
        model.lambda = 0
    end
    return warning
end

# keyword constructor
function RidgeRegressor(; target_type=Float64, lambda=0.0)

    model = RidgeRegressor(target_type, lambda)

    message = MLJBase.clean!(model)
    isempty(message) || @warn message

    return model
    
end</code></pre><h3><a class="nav-anchor" id="Supervised-models-1" href="#Supervised-models-1">Supervised models</a></h3><p>Below we describe the compulsory and optional methods to be specified for each concrete type <code>SomeSupervisedModel{R} &lt;: MLJBase.Supervised{R}</code>. We restrict attention to algorithms handling a <em>single</em> (univariate) target. Differences in the multivariate case are described later.</p><h4><a class="nav-anchor" id="The-form-of-data-for-fitting-and-prediction,-and-the-ouput_kind-trait-1" href="#The-form-of-data-for-fitting-and-prediction,-and-the-ouput_kind-trait-1">The form of data for fitting and prediction, and the ouput_kind trait</a></h4><p>The argument <code>X</code> passed to the <code>fit</code> method described below, and the argument <code>Xnew</code> of the <code>predict</code> (or <code>transform</code>) method, are arbitrary tables. Here <em>table</em> means an object supporting the<a href="https://github.com/JuliaData/Tables.jl">Tables.jl</a> interface. If the core algorithm requires data in a different or more specific form, then <code>fit</code> will need to coerce the table into the form desired. To this end, MLJ provides the convenience method <code>MLJBase.matrix</code>; <code>MLJBase.matrix(Xtable)</code> is a two-dimensional <code>Array{T}</code> where <code>T</code> is the tightest common type of elements of <code>Xtable</code>, and <code>Xtable</code> is any table. </p><p>Other convenience methods provided by MLJBase for handling tabular data are: <code>selectrows</code>, <code>selectcols</code>, <code>schema</code> (for extracting the size, names and eltypes of a table) and <code>table</code> (for materializing an abstract matrix, or named tuple of vectors, as a table matching a given prototype). Query the doc-strings for details.</p><p>Note that generally the same type coercions applied to <code>X</code> by <code>fit</code> will need to be applied by <code>predict</code> to <code>Xnew</code>. </p><p><strong>Important convention</strong> It is to be understood that the columns of the table <code>X</code> correspond to features and the rows to patterns.</p><p>The form of the target data <code>y</code> passed to <code>fit</code> depends on the kind of supervised model. For &quot;regressors&quot;, <code>y</code> is a vector, for &quot;classifiers&quot; it must be a categorical vector. More precisely, the form is determined by value returned by the trait <code>output_kind</code> that each model type should define (see below):</p><table><tr><th><code>output_kind</code> return value</th><th>type of <code>y</code></th></tr><tr><td><code>:continuous</code></td><td><code>Vector{&lt;:AbstractFloat}</code></td></tr><tr><td><code>:binary</code></td><td><code>CategoricalVector</code></td></tr><tr><td><code>:multiclass</code></td><td><code>CategoricalVector</code></td></tr><tr><td><code>:ordered_factor_finite</code></td><td><code>CategoricalVector</code></td></tr><tr><td><code>:ordered_factor_infinite</code></td><td><code>Vector{&lt;:Integer}</code></td></tr></table><h4><a class="nav-anchor" id="The-fit-method-1" href="#The-fit-method-1">The fit method</a></h4><p>A compulsory <code>fit</code> method returns three objects:</p><pre><code class="language-julia">MLJBase.fit(model::SomeSupervisedModelType, verbosity::Int, X, y) -&gt; fitresult, cache, report</code></pre><p>Note: The <code>Int</code> typing of <code>verbosity</code> cannot be omitted.</p><ol><li><p><code>fitresult::R</code> is the fit-result in the sense above (which becomes an  argument for <code>predict</code> discussed below).</p></li><li><p><code>report</code> is either a <code>Dict{Symbol,Any}</code> object, or <code>nothing</code> if  there is nothing to report. So for example, <code>fit</code> might declare  <code>report[:feature_importances] = ...</code>.  Any training-related  statistics, such as internal estimates of the generalization  error, feature rankings, and coefficients in linear models, should  be returned in the <code>report</code> dictionary. How, or if, these are  generated should be controlled by hyperparameters (the fields of  <code>model</code>). Reports get merged with those generated by previous  calls to <code>fit</code> by MLJ.</p></li></ol><p>3.	The value of <code>cache</code> can be <code>nothing</code>, unless one is also defining an     <code>update</code> method (see below). The Julia type of <code>cache</code> is not presently restricted.</p><p>It is not necessary for <code>fit</code> to provide dimension checks or to call <code>clean!</code> on the model; MLJ will carry out such checks.</p><p>The method <code>fit</code> should never alter hyperparameter values. If the package is able to suggest better hyperparameters, as a byproduct of training, return these in the report field.</p><p>One should test that actual fit-results have the type declared in the model <code>mutable struct</code> declaration. To help with this, <code>MLJBase.fitresult_type(m)</code> returns the declared type, for any supervised model (or model type) <code>m</code>.</p><p>The <code>verbosity</code> level (0 for silent) is for passing to learning algorithm itself. A <code>fit</code> method wrapping such an algorithm should generally avoid doing any of its own logging.</p><h4><a class="nav-anchor" id="The-predict-method-1" href="#The-predict-method-1">The predict method</a></h4><p>The compulsory predict method has the form</p><pre><code class="language-julia">MLJBase.predict(model::SomeSupervisedModelType, fitresult, Xnew) -&gt; yhat</code></pre><p>Here <code>Xnew</code> is an arbitrary table (see above).</p><p><strong>Prediction types for deterministic responses.</strong> In the case of <code>Deterministic</code> models, <code>yhat</code> must have the same type as the target <code>y</code> passed to the <code>fit</code> method (see above discussion on the form of data for fitting), with one exception: If predicting an infinite ordered factor (where <code>fit</code> receives a <code>Vector{&lt;:Integer}</code> object) the prediction may be continuous, i.e., of type <code>Vector{&lt;:AbstractFloat}</code>. For all other classifiers, the categorical vector returned by <code>predict</code> <strong>must have the levels in the categorical pool of the target data presented in training</strong>, even if not all levels appear in the training data or prediction itself. That is, we must have <code>levels(yhat) == levels(y)</code>.</p><p>Unfortunately, code not written with the preservation of categorical levels in mind poses special problems. To help with this, MLJ provides a utility <code>CategoricalDecoder</code> which can decode a <code>CategoricalArray</code> into a plain array, and re-encode a prediction with the original levels intact. The <code>CategoricalDecoder</code> object created during <code>fit</code> will need to be bundled with <code>fitresult</code> to make it available to <code>predict</code> during re-encoding.</p><p>So, for example, if the core algorithm being wrapped by <code>fit</code> expects a nominal target <code>yint</code> of type <code>Vector{Int64}</code> then the <code>fit</code> method may look something like this:</p><pre><code class="language-julia">function MLJBase.fit(model::SomeSupervisedModelType, verbosity, X, y)
    decoder = MLJBase.CategoricalDecoder(y, Int64)
    yint = transform(decoder, y)
    core_fitresult = SomePackage.fit(X, yint, verbosity=verbosity)
    fitresult = (decoder, core_fitresult)
    cache = nothing
    report = nothing
    return fitresult, cache, report
end</code></pre><p>while the corresponding <code>predict</code> operation might look like this:</p><pre><code class="language-julia">function MLJBase.predict(model::SomeSupervisedModelType, fitresult, Xnew)
    decoder, core_fitresult = fitresult
    yhat = SomePackage.predict(core_fitresult, Xnew)
    return inverse_transform(decoder, yhat)
end</code></pre><p>Query <code>?MLJBase.DecodeCategorical</code> for more information.</p><p><strong>Prediction types for probabilistic responses.</strong> In the case of <code>Probabilistic</code> models, <code>yhat</code> must be a <code>Vector</code> whose elements are distributions (one distribution per row of <code>Xnew</code>).</p><p>A <em>distribution</em> is any instance of a subtype of <code>Distributions.Distribution</code> from the package Distributions.jl, or any instance of the additional types <code>UnivariateNominal</code> and <code>MultivariateNominal</code> defined in MLJBase.jl (or any other type <code>D</code> for which <code>MLJBase.isdistribution(::D) = true</code>, meaning <code>Base.rand</code> and <code>Distributions.pdf</code> are implemented, as well <code>Distributions.mean</code>/<code>Distribution.median</code> or <code>Distributions.mode</code>).</p><p>Use <code>UnivariateNominal</code> for <code>Probabilistic</code> classifiers with a single nominal target, whether binary or multiclass. For example, suppose <code>levels(y)=[&quot;yes&quot;, &quot;no&quot;, &quot;maybe&quot;]</code> and set <code>L=levels(y)</code>. Then, if the predicted probabilities for some input pattern are <code>[0.1, 0.7, 0.2]</code>, respectively, then the prediction returned for that pattern will be <code>UnivariateNominal(L, [0.1, 0.7, 0.2])</code>. Query <code>?UnivariateNominal</code> for more information. </p><p>The <code>predict</code> method will need access to all levels of the target variable presented <code>y</code> presented for training, which consequently need to be encoded in the <code>fitresult</code> returned by <code>fit</code>. If a <code>CategoricalDecoder</code> object, <code>decoder</code>, has been bundled in <code>fitresult</code>, as in the deterministic example above, then the levels are given by <code>levels(decoder)</code>.</p><pre><code class="language-none">

#### Trait declarations

There are a number of recommended trait declarations for each concrete
subtype `SomeSupervisedModel &lt;: Supervised`. Basic fitting, resampling
and tuning in MLJ does not require these traits but some advanced MLJ
meta-algorithms may require them now, or in the future. In particular,
MLJ&#39;s `models(::Task)` method (matching models to user-specified
tasks) can only identify models having a complete set of trait
declarations. A full set of declarations are shown below for the
`RidgeRegressor` type:
</code></pre><p>julia MLJBase.output<em>kind(::Type{&lt;:RidgeRegressor}) = :continuous MLJBase.output</em>quantity(::Type{&lt;:RidgeRegressor}) = :univariate MLJBase.input<em>kinds(::Type{&lt;:RidgeRegressor}) = [:continuous, ] MLJBase.input</em>quantity(::Type{&lt;:RidgeRegressor}) = :multivariate MLJBase.is<em>pure</em>julia(::Type{&lt;:RidgeRegressor}) = :yes MLJBase.load<em>path(::Type{&lt;:RidgeRegressor}) = &quot;MLJ.RidgeRegressor&quot; MLJBase.package</em>name(::Type{&lt;:RidgeRegressor}) = &quot;MultivariateStats&quot; MLJBase.package<em>uuid(::Type{&lt;:RidgeRegressor}) = &quot;6f286f6a-111f-5878-ab1e-185364afe411&quot; MLJBase.package</em>url((::Type{&lt;:RidgeRegressor}) = &quot;https://github.com/JuliaStats/MultivariateStats.jl&quot;</p><pre><code class="language-none">
method                   | return type       | declarable return values | default value
-------------------------|-------------------|---------------------------|-------------------
`output_kind`            | `Symbol`          |`:continuous`, `:binary`, `:multiclass`, `:ordered_factor_finite`, `:ordered_factor_infinite`, `:mixed` | `:unknown`
`output_quantity`        | `Symbol`          |`:univariate`, `:multivariate`| `:univariate`
`input_kinds`          | `Vector{Symbol}`  | one or more of: `:continuous`, `:multiclass`, `:ordered_factor_finite`, `:ordered_factor_infinite`, `:missing` | `Symbol[]`
`input_quantity`        | `Symbol`          | `:univariate`, `:multivariate` | `:multivariate`
`is_pure_julia`          | `Symbol`          | `:yes`, `:no`             | `:unknown`
`load_path`              | `String`          | unrestricted              | &quot;unknown&quot;
`package_name`           | `String`          | unrestricted              | &quot;unknown&quot;
`package_uuid`           | `String`          | unrestricted              | &quot;unknown&quot;
`package_url`            | `String`          | unrestricted              | &quot;unknown&quot;

Note that `:binary` does not mean *boolean*. Rather, it
means the model is a classifier but is unable to classify targets with more than two
classes. As explained above, all classifiers are passed training targets
as `CategoricalVector`s, whose element types are
arbitrary.

The option `:mixed` for `output_kind` is intended primarily
for transformers, such as MLJ&#39;s built-in `FeatureSelector`.

You can test declarations of traits by calling `info(SomeModelType)`.

A `clean!` method may optionally be overloaded (the default returns an
empty message without changing model fields):
This method is for checking and correcting invalid fields
(hyperparameters) of the model, returning a warning `message`
explaining what has been changed. It should only throw an exception as
a last resort. This method should be called by the model keyword
constructor, as shown in the example below, and is called by MLJ
before each call to `fit`.


#### The update! method

An `update` method may be overloaded to enable a call by MLJ to
retrain a model (on the same training data) to avoid repeating
computations unnecessarily.
</code></pre><p>julia MLJBase.update(model::SomeSupervisedModelType, verbosity, old<em>fitresult, old</em>cache, X, y) -&gt; fitresult, cache, report ````</p><p>If an MLJ <code>Machine</code> is being <code>fit!</code> and it is not the first time, then <code>update</code> is called instead of <code>fit</code> unless <code>fit!</code> has been called with new rows. However, <code>MLJBase</code> defines a fallback for <code>update</code> which just calls <code>fit</code>. For context, see <a href="../internals/">&quot;MLJ Internals&quot;</a>. Learning networks wrapped as models constitute one use-case: One would like each component model to be retrained only when hyperparameter changes &quot;upstream&quot; make this necessary. In this case MLJ provides a fallback (specifically, the fallback is for any subtype of <code>Supervised{Node}</code>). A second important use-case is iterative models, where calls to increase the number of iterations only restarts the iterative procedure if other hyperparameters have also changed. For an example see <code>builtins/Ensembles.jl</code>.</p><p>In the event that the argument <code>fitresult</code> (returned by a preceding call to <code>fit</code>) is not sufficient for performing an update, the author can arrange for <code>fit</code> to output in its <code>cache</code> return value any additional information required, as this is also passed as an argument to the <code>update</code> method.</p><h4><a class="nav-anchor" id="Multivariate-models-1" href="#Multivariate-models-1">Multivariate models</a></h4><p>TODO</p><p>&lt;!– ##  Checklist for new adding models  –&gt;</p><p>&lt;!– At present the following checklist is just for supervised models in –&gt; &lt;!– lazily loaded interface implementations. –&gt;</p><p>&lt;!– - Copy and edit file –&gt; &lt;!– <a href="../src/interfaces/DecisionTree.jl">&quot;src/interfaces/DecisionTree.jl&quot;</a> –&gt; &lt;!– which is annotated for use as a template. Give your new file a name –&gt; &lt;!– identical to the package name, including &quot;.jl&quot; extension, such as –&gt; &lt;!– &quot;DecisionTree.jl&quot;. Put this file in &quot;src/interfaces/&quot;. –&gt;</p><p>&lt;!– - Register your package for lazy loading with MLJ by finding out the –&gt; &lt;!– UUID of the package and adding an appropriate line to the <code>__init__</code> –&gt; &lt;!– method at the end of &quot;src/MLJ.jl&quot;. It will look something like this: –&gt;</p><p>&lt;!– <span>$julia --&gt; &lt;!-- function __init__() --&gt; &lt;!--    @load_interface DecisionTree &quot;7806a523-6efd-50cb-b5f6-3fa6f1930dbb&quot; lazy=true --&gt; &lt;!--    @load_interface NewExternalPackage &quot;893749-98374-9234-91324-1324-9134-98&quot; lazy=true --&gt; &lt;!-- end --&gt; &lt;!--$</span> –&gt;</p><p>&lt;!– With <code>lazy=true</code>, your glue code only gets loaded by the MLJ user –&gt; &lt;!– after they run <code>import NewExternalPackage</code>. For testing in your local –&gt; &lt;!– MLJ fork, you may want to set <code>lazy=false</code> but to use <code>Revise</code> you –&gt; &lt;!– will also need to move the <code>@load_interface</code> line out outside of the –&gt; &lt;!– <code>__init__</code> function.  –&gt;</p><p>&lt;!– - Write self-contained test-code for the methods defined in your glue –&gt; &lt;!– code, in a file with name like &quot;TestExternalPackage.jl&quot;, but –&gt; &lt;!– placed in &quot;test/&quot;. This code should be wrapped in a module to prevent –&gt; &lt;!– namespace conflicts with other test code. For a module name, just –&gt; &lt;!– prepend &quot;Test&quot;, as in &quot;TestDecisionTree&quot;. See &quot;test/TestDecisionTree.jl&quot; –&gt; &lt;!– for an example. –&gt;</p><p>&lt;!– - Do not add the external package to the <code>Project.toml</code> file in the –&gt; &lt;!–   usual way. Rather, add its UUID to the <code>[extras]</code> section of –&gt; &lt;!–   <code>Project.toml</code> and add the package name to <code>test = [Test&quot;, &quot;DecisionTree&quot;, --&gt; &lt;!--   ...]</code>. –&gt;</p><p>&lt;!– - Add suitable lines to <a href="../test/runtests.jl">&quot;test/runtests.jl&quot;</a> to –&gt; &lt;!– <code>include</code> your test file, for the purpose of testing MLJ core and all –&gt; &lt;!– currently supported packages, including yours. You can Test your code –&gt; &lt;!– by running <code>test MLJ</code> from the Julia interactive package manager. You –&gt; &lt;!– will need to <code>Pkg.dev</code> your local MLJ fork first. To test your code in –&gt; &lt;!– isolation, locally edit &quot;test/runtest.jl&quot; appropriately. –&gt;</p><h3><a class="nav-anchor" id="Unsupervised-models-1" href="#Unsupervised-models-1">Unsupervised models</a></h3><p>&lt;!– TODO:</p><ul><li>specify that the output of <code>predict</code>/<code>transform</code> should be a table</li></ul><p>–&gt;</p><footer><hr/><a class="next" href="../"><span class="direction">Next</span><span class="title">MLJ.jl</span></a></footer></article></body></html>
